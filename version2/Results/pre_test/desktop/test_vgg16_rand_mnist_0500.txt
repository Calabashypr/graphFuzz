
analyse the exceptions in iter:6
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 145., 255., 211.,  31.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            32., 237., 253., 252.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            11., 175., 253., 252.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 144., 253., 252.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            16., 191., 253., 252.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            26., 221., 253., 252., 124.,  31.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 125., 253., 252., 252., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 253., 252., 252., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 255., 253., 253., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 253., 252., 252., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 253., 252., 252., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 253., 252., 252., 108.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 255., 253., 253., 170.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 253., 252., 252., 252.,  42.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 149., 252., 252., 252., 144.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 109., 252., 252., 252., 144.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 218., 253., 253., 255.,  35.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 175., 252., 252., 253.,  35.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,  73., 252., 252., 253.,  35.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,  31., 211., 252., 253.,  35.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:6

analyse output arrays in iter:8

pre layer res:
6:relu
{'name': 'relu', 'output': array([[[[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]],

        [[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]],

        [[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]],

        ...,

        [[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]],

        [[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]],

        [[      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [      0.,       0.,       0., ...,       0.,       0.,
                0.],
         [ 514176.,  822656.,  925440., ...,  925440.,  822656.,
           514176.],
         ...,
         [3087360., 4939776., 5557248., ..., 5557248., 4939776.,
          3087360.],
         [1543680., 2469888., 2778624., ..., 2778624., 2469888.,
          1543680.],
         [ 514560.,  823296.,  926208., ...,  926208.,  823296.,
           514560.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 100]), 'from': [5], 'to': [17]}
tf node:
{'name': 'cos', 'output': array([[[[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]],

        ...,

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.5774741 ,  0.11824997, -0.5905013 , ..., -0.5905013 ,
           0.11824997, -0.5774741 ],
         ...,
         [-0.7897536 ,  0.11389711, -0.9976932 , ..., -0.9976932 ,
           0.11389711, -0.7897536 ],
         [-0.3242271 ,  0.74628985, -0.03396178, ..., -0.03396178,
           0.74628985, -0.3242271 ],
         [ 0.10984276, -0.6935276 , -0.87163115, ..., -0.87163115,
          -0.6935276 ,  0.10984276]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 100]), 'from': [6], 'to': [7]}
ms node:
{'name': 'cos', 'output': array([[[[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]],

        ...,

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.9335061 ,  0.768134  ,  0.99441785, ...,  0.99441785,
           0.7266422 , -0.9335061 ],
         ...,
         [-0.26133394, -0.9125679 ,  0.9274111 , ...,  0.9274111 ,
          -0.9125679 , -0.492016  ],
         [ 0.50205714,  0.85346484, -0.29559186, ..., -0.29559186,
           0.85346484,  0.50205714],
         [-0.68405527,  0.3431933 ,  0.3199561 , ...,  0.3199561 ,
           0.3431933 , -0.68405527]]]], dtype=float32), 'output_shape': (1, 512, 28, 100), 'from': [6], 'to': [7]}
torch node:
{'name': 'cos', 'output': array([[[[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]],

        ...,

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]],

        [[ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [ 1.        ,  1.        ,  1.        , ...,  1.        ,
           1.        ,  1.        ],
         [-0.29343668,  0.03410041, -0.49241605, ..., -0.49241605,
           0.03410041, -0.23315398],
         ...,
         [-0.9871599 , -0.75740594, -0.35343158, ..., -0.35343158,
           0.90891266, -0.22916386],
         [-0.9826753 ,  0.7213733 ,  0.9916745 , ...,  0.9916745 ,
          -0.6397837 , -0.9981148 ],
         [ 0.43988   ,  0.5152318 ,  0.85869825, ...,  0.85869825,
           0.6659586 ,  0.49511316]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 100]), 'from': [6], 'to': [7]}

generate models:8

analyse the exceptions in iter:9
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            189., 190.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 143.,
            247., 153.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 136., 247.,
            242.,  86.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 252.,
            187.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  62., 185.,  18.,   0.,   0.,   0.,   0.,  89., 236., 217.,
             47.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 216., 253.,  60.,   0.,   0.,   0.,   0., 212., 255.,  81.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 206., 252.,  68.,   0.,   0.,   0.,  48., 242., 253.,  89.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            131., 251., 212.,  21.,   0.,   0.,  11., 167., 252., 197.,   5.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29.,
            232., 247.,  63.,   0.,   0.,   0., 153., 252., 226.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  45., 219.,
            252., 143.,   0.,   0.,   0., 116., 249., 252., 103.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,  96., 253., 255.,
            253., 200., 122.,   7.,  25., 201., 250., 158.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  92., 252., 252., 253.,
            217., 252., 252., 200., 227., 252., 231.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  87., 251., 247., 231.,  65.,
             48., 189., 252., 252., 253., 252., 251., 227.,  35.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 190., 221.,  98.,   0.,   0.,
              0.,  42., 196., 252., 253., 252., 252., 162.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 111.,  29.,   0.,   0.,   0.,
              0.,  62., 239., 252.,  86.,  42.,  42.,  14.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             15., 148., 253., 218.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            121., 252., 231.,  28.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31.,
            221., 251., 129.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 218.,
            252., 160.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 122.,
            252.,  82.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:9

final statics:
total operators:28
tensorflow --> nums:1,distinct_bugs:1
mindspore --> nums:3,distinct_bugs:2
torch --> nums:3,distinct_bugs:2
tensorflow --> 
cos:1
mindspore --> 
conv2d:2
cos:1
torch --> 
conv2d:2
cos:1

generate models:9

analyse output arrays in iter:23

pre layer res:
7:add
{'name': 'add', 'output': array([[[[3.6989797e-02, 3.6989797e-02, 3.6989797e-02, ...,
          3.6989797e-02, 3.6989797e-02, 3.6989797e-02],
         [3.6989797e-02, 3.6989797e-02, 3.6989797e-02, ...,
          3.6989797e-02, 3.6989797e-02, 3.6989797e-02],
         [1.2755103e-03, 1.2755103e-03, 1.2755103e-03, ...,
          1.2755103e-03, 1.2755103e-03, 1.2755103e-03],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02]],

        [[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 1.0357143e+00, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02]],

        [[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [1.0357143e+00, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 5.3801865e-32],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          1.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02]],

        ...,

        [[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 1.0357143e+00, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02]],

        [[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [1.0357143e+00, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 5.3801865e-32],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          1.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02]],

        [[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, ...,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [1.2755103e-03, 1.2755103e-03, 1.2755103e-03, ...,
          1.2755103e-03, 1.2755103e-03, 1.2755103e-03],
         [1.2755103e-03, 1.2755103e-03, 1.2755103e-03, ...,
          1.2755103e-03, 1.2755103e-03, 1.2755103e-03],
         [3.6989797e-02, 3.6989797e-02, 3.6989797e-02, ...,
          3.6989797e-02, 3.6989797e-02, 3.6989797e-02]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 14, 14]), 'from': [2, 10], 'to': [5]}
tf node:
{'name': 'log', 'output': array([[[[-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00],
         [-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00],
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00]],

        [[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf,  0.0000000e+00, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322046e+00,  3.5091303e-02, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00]],

        [[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [ 3.5091303e-02, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000000e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
           0.0000000e+00,           -inf,           -inf],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00]],

        ...,

        [[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf,  0.0000000e+00, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322046e+00,  3.5091303e-02, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00]],

        [[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [ 3.5091303e-02, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000000e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
           0.0000000e+00,           -inf,           -inf],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00]],

        [[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00, ...,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         [-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00]]]],
      dtype=float32), 'output_shape': TensorShape([1, 64, 14, 14]), 'from': [7], 'to': [12]}
ms node:
{'name': 'log', 'output': array([[[[-3.2971146e+00, -3.2971146e+00, -3.2971146e+00, ...,
          -3.2971146e+00, -3.2971146e+00, -3.2971146e+00],
         [-3.2971146e+00, -3.2971146e+00, -3.2971146e+00, ...,
          -3.2971146e+00, -3.2971146e+00, -3.2971146e+00],
         [-6.6644063e+00, -6.6644063e+00, -6.6644063e+00, ...,
          -6.6644063e+00, -6.6644063e+00, -6.6644063e+00],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00]],

        [[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf, -1.4305115e-06, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322077e+00,  3.5094716e-02, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00]],

        [[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [ 3.5094716e-02, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000008e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
          -1.4305115e-06,           -inf,           -inf],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00]],

        ...,

        [[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf, -1.4305115e-06, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322077e+00,  3.5094716e-02, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00]],

        [[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [ 3.5094716e-02, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000008e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
          -1.4305115e-06,           -inf,           -inf],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00]],

        [[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00, ...,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [-6.6644063e+00, -6.6644063e+00, -6.6644063e+00, ...,
          -6.6644063e+00, -6.6644063e+00, -6.6644063e+00],
         [-6.6644063e+00, -6.6644063e+00, -6.6644063e+00, ...,
          -6.6644063e+00, -6.6644063e+00, -6.6644063e+00],
         [-3.2971146e+00, -3.2971146e+00, -3.2971146e+00, ...,
          -3.2971146e+00, -3.2971146e+00, -3.2971146e+00]]]],
      dtype=float32), 'output_shape': (1, 64, 14, 14), 'from': [7], 'to': [12]}
torch node:
{'name': 'log', 'output': array([[[[-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00],
         [-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00],
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00]],

        [[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf,  0.0000000e+00, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322043e+00,  3.5091303e-02, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00]],

        [[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [ 3.5091303e-02, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000000e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
           0.0000000e+00,           -inf,           -inf],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00]],

        ...,

        [[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [          -inf,           -inf,  0.0000000e+00, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [-3.3322043e+00,  3.5091303e-02, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00]],

        [[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [ 3.5091303e-02, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf, -7.2000000e+01],
         ...,
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         [          -inf,           -inf,           -inf, ...,
           0.0000000e+00,           -inf,           -inf],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00]],

        [[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00, ...,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00],
         [          -inf,           -inf,           -inf, ...,
                    -inf,           -inf,           -inf],
         ...,
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         [-6.6644092e+00, -6.6644092e+00, -6.6644092e+00, ...,
          -6.6644092e+00, -6.6644092e+00, -6.6644092e+00],
         [-3.2971132e+00, -3.2971132e+00, -3.2971132e+00, ...,
          -3.2971132e+00, -3.2971132e+00, -3.2971132e+00]]]],
      dtype=float32), 'output_shape': torch.Size([1, 64, 14, 14]), 'from': [7], 'to': [12]}

generate models:19

analyse the exceptions in iter:24
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 166., 222.,  55.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 197., 254., 218.,   5.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  29., 249., 254., 254.,   9.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  45., 254., 254., 174.,   2.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   4., 164., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 146., 254., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 101., 245., 254., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             97., 248., 254., 204., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  12.,  59.,  98., 151., 237.,
            254., 254., 109.,  35., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  41., 216., 254., 254., 239.,
            153.,  37.,   4.,  32., 254., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   7.,  44.,  44.,  30.,
              0.,   0.,   0.,  32., 254., 254.,  96.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  19., 230., 254., 174.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 197., 254., 110.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 197., 254.,  85.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 197., 253.,  63.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  37.,
             54.,  54.,  45.,  26.,  84., 221.,  84.,  21.,  31., 162.,  78.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,  41., 141., 244.,
            254., 254., 248., 236., 254., 254., 254., 233., 239., 254., 138.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  23., 167., 254., 254., 254.,
            254., 229., 228., 185., 138., 138., 138., 138., 138., 138.,  44.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 113., 254., 254., 254., 179.,
             64.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  32., 209., 183.,  97.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 100x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:20

analyse output arrays in iter:25

pre layer res:
4:conv2d
{'name': 'conv2d', 'output': array([[[[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]],

        [[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]],

        [[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]],

        ...,

        [[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]],

        [[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]],

        [[16.253906 , 26.53711  , 15.624023 , ..., 26.672852 ,
          25.885742 , 15.435059 ],
         [20.729004 , 34.479004 , 19.09082  , ..., 35.242188 ,
          30.36084  , 19.910156 ],
         [12.85498  , 21.691895 ,  8.836914 , ..., 30.056152 ,
          16.486816 , 12.036133 ],
         ...,
         [ 3.4667969,  4.4067383,  5.9399414, ..., 11.408691 ,
           3.4667969,  3.4667969],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  4.4067383,
           5.9399414,  5.9399414],
         [ 5.9399414, 11.879883 , 20.390625 , ...,  0.9399414,
           5.9399414,  5.9399414]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 100]), 'from': [3], 'to': [18]}
tf node:
{'name': 'sin', 'output': array([[[[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]],

        [[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]],

        [[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]],

        ...,

        [[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]],

        [[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]],

        [[-0.5192242 ,  0.98618275,  0.08384129, ...,  0.99952924,
           0.6838315 ,  0.26952973],
         [ 0.952744  ,  0.07843464,  0.23893058, ..., -0.6324161 ,
          -0.869943  ,  0.87264884],
         [ 0.28461987,  0.29480746,  0.5545848 , ..., -0.97781736,
          -0.7024636 , -0.5057385 ],
         ...,
         [-0.31950232, -0.95365137, -0.33654353, ..., -0.9158739 ,
          -0.31950232, -0.31950232],
         [-0.33654353, -0.6338245 ,  0.99955815, ..., -0.95365137,
          -0.33654353, -0.33654353],
         [-0.33654353, -0.6338245 ,  0.99955815, ...,  0.80752355,
          -0.33654353, -0.33654353]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 100]), 'from': [4], 'to': [5]}
ms node:
{'name': 'sin', 'output': array([[[[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]],

        [[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]],

        [[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]],

        ...,

        [[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]],

        [[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]],

        [[-0.51919323,  0.98607415,  0.08327204, ...,  0.9995174 ,
           0.68467855,  0.2693672 ],
         [ 0.95260084,  0.07750668,  0.24053603, ..., -0.63172144,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496543,  0.29284942,  0.5531794 , ..., -0.9775991 ,
          -0.70357186, -0.50525135],
         ...,
         [-0.32052645, -0.95388764, -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.634281  ,  0.99957865, ..., -0.95388764,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.634281  ,  0.99957865, ...,  0.8073497 ,
          -0.33682102, -0.33682102]]]], dtype=float32), 'output_shape': (1, 256, 14, 100), 'from': [4], 'to': [5]}
torch node:
{'name': 'sin', 'output': array([[[[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]],

        [[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]],

        [[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]],

        ...,

        [[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]],

        [[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]],

        [[-0.5191949 ,  0.9860745 ,  0.08327204, ...,  0.9995174 ,
           0.6846785 ,  0.26936626],
         [ 0.95260084,  0.07749908,  0.24053234, ..., -0.6317244 ,
          -0.8691197 ,  0.8729783 ],
         [ 0.28496632,  0.29285488,  0.5531794 , ..., -0.9775991 ,
          -0.7035705 , -0.5052506 ],
         ...,
         [-0.32052645, -0.9538875 , -0.33682102, ..., -0.91479933,
          -0.32052645, -0.32052645],
         [-0.33682102, -0.63428026,  0.99957865, ..., -0.9538875 ,
          -0.33682102, -0.33682102],
         [-0.33682102, -0.63428026,  0.99957865, ...,  0.8073496 ,
          -0.33682102, -0.33682102]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 100]), 'from': [4], 'to': [5]}

generate models:21

analyse the exceptions in iter:37
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsafeViewBackward0>)]}
Given groups=1, weight of size [128, 256, 3, 3], expected input[1, 1, 28, 100] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:31

analyse the exceptions in iter:49
torch exception:
{'id': 4, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]]], grad_fn=<ExpBackward0>)]}
Given groups=1, weight of size [512, 512, 3, 3], expected input[1, 256, 28, 28] to have 512 channels, but got 256 channels instead
mindspore exception:
{'id': 4, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  ...
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:41

final statics:
total operators:28
tensorflow --> nums:3,distinct_bugs:3
mindspore --> nums:7,distinct_bugs:4
torch --> nums:7,distinct_bugs:4
tensorflow --> 
cos:1
log:1
sin:1
mindspore --> 
conv2d:4
cos:1
log:1
linear:1
torch --> 
conv2d:4
cos:1
log:1
linear:1

generate models:41

analyse output arrays in iter:56

pre layer res:
7:exp
{'name': 'exp', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [3], 'to': [4]}
tf node:
{'name': 'conv2d', 'output': array([[[[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        ...,

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [7], 'to': [9]}
ms node:
{'name': 'conv2d', 'output': array([[[[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        ...,

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]]]], dtype=float32), 'output_shape': (1, 256, 28, 28), 'from': [7], 'to': [9]}
torch node:
{'name': 'conv2d', 'output': array([[[[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        ...,

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]],

        [[ 512.,  768.,  768., ...,  768.,  768.,  512.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         ...,
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 768., 1152., 1152., ..., 1152., 1152.,  768.],
         [ 512.,  768.,  768., ...,  768.,  768.,  512.]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 28]), 'from': [7], 'to': [9]}

pre layer res:
1:relu
{'name': 'relu', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [8], 'to': [23]}
tf node:
{'name': 'log', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [1], 'to': [2]}
ms node:
{'name': 'log', 'output': array([[[[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]],

        [[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]],

        [[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]],

        ...,

        [[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]],

        [[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]],

        [[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         ...,
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06, ...,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06]]]],
      dtype=float32), 'output_shape': (1, 128, 28, 28), 'from': [1], 'to': [2]}
torch node:
{'name': 'log', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 28, 28]), 'from': [1], 'to': [2]}

generate models:46

analyse the exceptions in iter:58
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]],

         [[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]],

         [[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]],

         ...,

         [[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]],

         [[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]],

         [[0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          ...,
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357],
          [0.0357, 0.0357, 0.0357,  ..., 0.0357, 0.0357, 0.0357]]]],
       grad_fn=<SoftmaxBackward0>)]}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 128, 28, 28] to have 256 channels, but got 128 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 128, 28, 28], dtype=Float32, value=
[[[[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]],
  [[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]],
  [[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]],
  ...
  [[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]],
  [[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]],
  [[3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   ...
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002],
   [3.57142873e-002, 3.57142873e-002, 3.57142873e-002 ... 3.57142873e-002, 3.57142873e-002, 3.57142873e-002]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 128, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:48

analyse output arrays in iter:64

pre layer res:
15:log
{'name': 'log', 'output': array([[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,
        -inf, -inf, -inf]], dtype=float32), 'output_shape': TensorShape([1, 14]), 'from': [16], 'to': [10]}
tf node:
{'name': 'cos', 'output': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan]], dtype=float32), 'output_shape': TensorShape([1, 14]), 'from': [15], 'to': [7]}
ms node:
{'name': 'cos', 'output': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan]], dtype=float32), 'output_shape': (1, 14), 'from': [15], 'to': [7]}
torch node:
{'name': 'cos', 'output': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan]], dtype=float32), 'output_shape': torch.Size([1, 14]), 'from': [15], 'to': [7]}

generate models:52

analyse output arrays in iter:66

pre layer res:
9:sin
{'name': 'sin', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [5], 'to': [6]}
tf node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [9], 'to': []}
ms node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': (1, 512, 28, 28), 'from': [9], 'to': []}
torch node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 28]), 'from': [9], 'to': []}

generate models:54

analyse the exceptions in iter:75
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  62.,  91., 213., 255., 228.,  91.,  12.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  70., 230., 253., 253., 253., 253., 253., 152.,   7.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 246., 253., 253., 253., 253., 253., 253., 253., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  21., 247., 253., 253., 253., 253., 253., 253., 208.,  24.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 156., 253., 253., 253., 253., 253., 253., 253., 195.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             88., 238., 253., 253., 253., 221., 253., 253., 253., 195.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            230., 253., 253., 253., 198.,  40., 177., 253., 253., 195.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  56., 156.,
            251., 253., 189., 182.,  15.,   0.,  86., 240., 253., 210.,  28.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 213., 253.,
            253., 156.,   3.,   0.,   0.,   0.,   0., 205., 253., 253., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 121., 252., 253.,
            135.,   3.,   0.,   0.,   0.,   0.,   0.,  46., 253., 253., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  28., 212., 253., 248.,
             23.,   0.,   0.,   0.,   0.,   0.,   0.,  42., 253., 253., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 197., 253., 234.,  70.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  42., 253., 253., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  11., 202., 253., 187.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  58., 253., 210.,  27.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 107., 253., 253.,  40.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  53., 227., 253., 195.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 107., 253., 253.,  40.,   0.,
              0.,   0.,   0.,   0.,   0.,  47., 227., 253., 231.,  58.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 107., 253., 253.,  40.,   0.,
              0.,   0.,   0.,   5., 131., 222., 253., 231.,  59.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  14., 204., 253., 226., 222.,
             73.,  58.,  58., 170., 253., 253., 227.,  58.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 253.,
            253., 253., 253., 253., 253., 238.,  58.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  33., 179., 241., 253.,
            253., 253., 253., 250., 116.,  14.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75., 179.,
            253., 151.,  89.,  86.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 50x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 50. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 50](transpose_b=True).

generate models:62

analyse the exceptions in iter:84
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,  21., 214., 253., 152., 152., 152., 152., 152.,
            152., 152., 152., 254., 253., 254., 172., 152.,  71.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,  61., 213., 252., 253., 252., 253., 252., 253.,
            252., 253., 252., 213., 252., 253., 252., 253., 252.,  82.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  41., 102., 102., 102., 102.,  82.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  52., 253., 102.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 132., 252., 102.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 255., 253., 102.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  41., 253., 252.,  20.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 163., 254., 131.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  41., 243., 253.,  50.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 113., 253., 224.,  20.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  41., 233., 252.,  81.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 123., 254., 233.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  21., 223., 253., 111.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 152., 253., 224.,  20.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 233., 252., 162.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 102., 254., 253.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 183., 253., 171.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  11., 213., 254., 131.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  51., 252., 233.,  30.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  51., 253., 254., 213., 183.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  31., 232., 253., 212.,  20.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 100x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:69

analyse the exceptions in iter:87
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          ...,
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.]],

         [[  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          ...,
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.]],

         [[  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          ...,
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.],
          [  0.,   0.,   0.,  ..., 593., 713.,   0.]],

         ...,

         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          ...,
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          ...,
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],

         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          ...,
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]],
       grad_fn=<ReshapeAliasBackward0>)]}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 512, 28, 28] to have 256 channels, but got 512 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 5.93000000e+002, 7.13000000e+002, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:72

analyse the exceptions in iter:92
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 196.,  99.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   5.,  49.,
              0.,   0.,   0.,   0.,   0.,   0.,  34., 244.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 135.,
              0.,   0.,   0.,   0.,   0.,   0.,  40., 253.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 171., 150.,
              0.,   0.,   0.,   0.,   0.,   0.,  40., 253.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 254., 233.,
              0.,   0.,   0.,   0.,   0.,   0.,  77., 253.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255., 136.,
              0.,   0.,   0.,   0.,   0.,   0.,  77., 254.,  99.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 254., 135.,
              0.,   0.,   0.,   0.,   0.,   0., 123., 253.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 254., 135.,
              0.,   0.,   0.,   0.,   0.,   0., 136., 253.,  98.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16., 254., 135.,
              0.,   0.,   0.,   0.,   0.,   0., 136., 237.,   8.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 254., 135.,
              0.,   0.,  38.,  99.,  98.,  98., 219., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 196., 255., 208.,
            186., 254., 254., 255., 254., 254., 254., 254.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 105., 254., 253.,
            239., 180., 135.,  39.,  39.,  39., 237., 170.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 137.,  92.,
             24.,   0.,   0.,   0.,   0.,   0., 234., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  13., 237., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  79., 253., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  31., 242., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  61., 248., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 234., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 234., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 196., 155.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:76

analyse the exceptions in iter:98
torch exception:
{'id': 5, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]],

         [[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]],

         [[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]],

         ...,

         [[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]],

         [[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]],

         [[224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          ...,
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594],
          [224603.8594, 224603.8594, 224603.8594,  ..., 224603.8594,
           224603.8594, 224603.8594]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 64, 14, 14] to have 512 channels, but got 64 channels instead
mindspore exception:
{'id': 5, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 64, 14, 14], dtype=Float32, value=
[[[[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]],
  [[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]],
  [[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]],
  ...
  [[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]],
  [[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]],
  [[2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   ...
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603922e+005, 2.24603922e+005, 2.24603922e+005],
   [2.24603922e+005, 2.24603922e+005, 2.24603922e+005 ... 2.24603859e+005, 2.24603859e+005, 2.24603859e+005]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 64, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:82

final statics:
total operators:28
tensorflow --> nums:6,distinct_bugs:4
mindspore --> nums:15,distinct_bugs:4
torch --> nums:14,distinct_bugs:4
tensorflow --> 
cos:2
log:1
sin:1
conv2d:2
mindspore --> 
conv2d:8
cos:2
log:2
linear:3
torch --> 
conv2d:8
cos:2
log:1
linear:3

generate models:83

analyse the exceptions in iter:107
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<AddBackward0>)]}
Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 512, 28, 28] to have 128 channels, but got 512 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:90

analyse the exceptions in iter:115
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 154.,
            253.,  56.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 107., 247.,
            171.,  19.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 238.,
             38.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 151.,   0.,   0.,   0.,   0.,   0.,  26., 210., 252., 125.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            147., 253., 188.,  38.,   0.,   0.,   0.,  57., 253., 253.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 114.,
            234., 252., 150.,   0.,   0.,   0.,  13., 169., 252., 202.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 147., 234.,
            252., 164.,  25.,   0.,   0.,   0.,  13., 206., 252., 115.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 163., 253., 227.,
            103.,  15.,   0.,   0.,   0.,   0.,  76., 243., 214.,  28.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   7., 154., 253., 255., 134.,
             10.,  29.,  29.,  22.,  23.,  29., 204., 253., 178.,   4.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  82., 252., 252., 253., 196.,
            197., 252., 253., 234., 234., 252., 253., 252., 170.,   9.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  63., 196., 252., 253., 252.,
            252., 252., 253., 233., 234., 252., 253., 233.,  88.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,  28.,  91., 139.,
            139., 139.,  28.,  22.,  97., 252., 241.,  59.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   7., 204., 253., 151.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   7., 150., 252., 202.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  66., 252., 224.,  19.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 241., 252., 118.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 176., 254.,  84.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            108., 243., 159.,  28.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26.,
            197., 202.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 101.,
            209.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 100x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:95

analyse the exceptions in iter:117
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26.,
             47.,  47.,  47.,  47.,  47.,  47.,  47.,  47.,  47.,  36.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 143., 146., 205.,
            253., 253., 253., 253., 253., 253., 253., 253., 253., 227.,  53.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 102., 247., 253., 253., 253.,
            253., 253., 253., 253., 253., 253., 253., 253., 253., 253., 241.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 200., 253., 253., 247., 214.,
            238., 214., 214., 214., 214., 214., 248., 253., 253., 244., 115.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 200., 253., 253., 130.,   0.,
            154.,   0.,   0.,   0.,   0.,  68., 240., 253., 253., 191.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 133., 168., 112.,   4.,   0.,
              9.,   0.,   0.,  26., 100., 246., 253., 253., 253., 139.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   4., 112., 206., 253., 253., 253., 251., 151.,  10.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 112., 253., 253., 253., 253., 217., 115.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            125., 247., 253., 253., 246., 176.,  34.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  47., 177.,
            249., 253., 241., 230., 115.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  72., 250., 253., 253.,
            253., 238.,  63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  84., 241., 253., 253., 253.,
            230.,  68.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,
             70., 163.,  28.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  68., 236., 253., 253., 241., 153.,
             56.,   0.,   0.,   5.,  16.,  16.,   7.,  16.,  87., 169., 172.,
            253., 253., 202.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 171., 253., 253., 253.,  61.,   0.,
              0.,  85., 116., 157., 253., 253., 169., 253., 253., 253., 253.,
            253., 253., 182.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 255., 253., 253., 253., 225., 216.,
            216., 243., 253., 253., 253., 253., 253., 253., 253., 253., 251.,
            191., 158.,  15.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 152., 245., 250., 253., 253., 253.,
            253., 253., 253., 253., 253., 253., 246., 245., 245., 208.,  89.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 145., 227., 253.,
            253., 253., 197., 145., 145., 145.,  17.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  35.,  45.,
             45.,  45.,  22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:97

analyse the exceptions in iter:125
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  52., 241.,  86.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   3.,  13.,  13., 108., 137., 137., 137., 238., 254.,  24.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 121., 254., 254., 254., 254., 254., 254., 254., 254., 103.,
              2.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,
            149., 222., 254., 248., 229., 237., 254., 246., 184., 105., 241.,
             18.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 115.,
            254., 248., 134.,  75.,   0.,  91., 254.,  91.,   0.,   0.,  90.,
              7.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  83.,
            254., 143.,   0.,   0.,  63., 235., 224.,  37.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38.,
            254., 238.,  71.,  11., 204., 205.,  17.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31.,
            238., 254., 237., 194., 254., 155.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             71., 237., 254., 254., 178.,  16.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 174., 254., 254., 203.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             34., 235., 254., 254., 254., 149.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  12.,
            167., 254., 201., 198., 254., 238.,  75.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  78.,
            254., 214.,  15.,  12., 205., 254., 239.,  35.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  27., 239.,
            254., 162.,   0.,   0., 193., 254., 254.,  43.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 138., 254.,
            238.,  51.,   0.,   0., 193., 254., 225.,  27.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10., 194., 254.,
            101.,   0.,   0.,   0., 193., 254., 125.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  25., 254., 220.,
              9.,   0.,   0., 109., 249., 186.,   5.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 141., 254., 217.,
             20., 113., 201., 250., 254., 125.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 250., 254., 253.,
            245., 254., 254., 254., 131.,   3.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,  91., 255., 255.,
            255., 255., 166.,  39.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:103

analyse the exceptions in iter:129
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<AtanBackward0>)]}
Given groups=1, weight of size [512, 128, 1, 1], expected input[1, 1, 28, 100] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:107

analyse output arrays in iter:135

pre layer res:
4:dropout
{'name': 'dropout', 'output': array([[[[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        ...,

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 7, 7]), 'from': [3], 'to': [5]}
tf node:
{'name': 'conv2d', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 7, 7]), 'from': [4], 'to': []}
ms node:
{'name': 'conv2d', 'output': array([[[[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        ...,

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]]]], dtype=float32), 'output_shape': (1, 256, 7, 7), 'from': [4], 'to': []}
torch node:
{'name': 'conv2d', 'output': array([[[[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        ...,

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]],

        [[inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         ...,
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf],
         [inf, inf, inf, ..., inf, inf, inf]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 7, 7]), 'from': [4], 'to': []}

generate models:112

analyse output arrays in iter:147

pre layer res:
15:reshape
{'name': 'reshape', 'output': array([[[[ 1., inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
           1.,  1.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
           0.,  0.]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 14, 14]), 'from': [14], 'to': [8]}
tf node:
{'name': 'softmax', 'output': array([[[[       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 14, 14]), 'from': [15], 'to': [1]}
ms node:
{'name': 'softmax', 'output': array([[[[0.        ,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857]]]],
      dtype=float32), 'output_shape': (1, 1, 14, 14), 'from': [15], 'to': [1]}
torch node:
{'name': 'softmax', 'output': array([[[[       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,
          0.07142857, 0.07142857, 0.07142857, 0.07142857]]]],
      dtype=float32), 'output_shape': torch.Size([1, 1, 14, 14]), 'from': [15], 'to': [1]}

generate models:123

analyse the exceptions in iter:161

generate models:134

analyse the exceptions in iter:166
torch exception:
{'id': 16, 'name': 'flatten', 'frame_work': 'torch', 'input_datas': [tensor([15391.8369], grad_fn=<MeanBackward1>)]}
Dimension out of range (expected to be in range of [-1, 0], but got 1)

generate models:138

analyse the exceptions in iter:168
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  29., 253., 255., 253.,
           143., 113., 113., 114.,  50., 113., 113.,  50.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 214., 253., 252.,
           252., 252., 252., 253., 237., 252., 252., 237.,  86.,   9.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 110., 253., 252.,
           252., 252., 252., 253., 252., 252., 252., 252., 253.,  27.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  84.,  84.,
           115., 223., 223., 237., 252., 252., 252., 252., 253.,  27.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 113., 252., 252., 252., 252., 190.,  12.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 255., 253., 253., 253., 253.,  79.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 126., 253., 252., 252., 252., 127.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 140., 253., 252., 252., 245.,  87.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,  19., 215., 253., 252., 252., 208.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            32., 153., 252., 253., 252., 252.,  84.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            57., 253., 253., 255., 253., 196.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29.,
           200., 252., 252., 253., 252., 195.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 117.,
           252., 252., 252., 253., 252., 164.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 225.,
           252., 252., 252., 253., 242.,  49.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  63., 240.,
           252., 252., 252., 253.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 114., 253.,
           253., 253., 253., 255.,  27.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 113., 252.,
           252., 252., 252., 253.,  27.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  88., 246.,
           252., 252., 252., 253.,  27.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 209.,
           252., 252., 252., 240.,  24.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38.,
           221., 252., 252.,  63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [128, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:140

analyse the exceptions in iter:169
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,  42., 235., 255.,  84.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,  15., 132., 208., 253., 253., 171., 108.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             6., 177., 253., 253., 253., 253., 253., 242., 110.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
           151., 253., 253., 253., 253., 253., 253., 253., 139.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48.,
           208., 253., 253., 253., 253., 253., 253., 253., 139.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85.,
           253., 253., 253., 253., 236., 156., 184., 253., 148.,   6.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 141.,
           253., 253., 253., 253.,  27.,   0., 170., 253., 253.,  74.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 253.,
           253., 253., 253., 253.,  27.,   0., 170., 253., 253.,  74.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16., 186., 253.,
           253., 253., 242., 105.,   4.,   0., 170., 253., 253.,  94.,   1.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 141., 253., 253.,
           253., 242., 100.,   0.,   0.,   0., 170., 253., 253., 253.,   8.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 141., 253., 253.,
           253., 224.,   0.,   0.,   0.,   0., 170., 253., 253., 253.,   8.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  12., 158., 253., 253.,
           230.,  51.,   0.,   0.,   0.,   0.,  18., 237., 253., 253.,   8.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 253., 253., 218.,
            61.,   0.,   0.,   0.,   0.,   0.,   0., 236., 253., 253.,   8.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 253., 253., 168.,
             0.,   0.,   0.,   0.,   0.,   0.,   0., 110., 253., 132.,   3.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 253., 253., 168.,
             0.,   0.,   0.,   0.,   0.,  20., 174., 239., 147.,   5.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   5., 155., 253., 253., 168.,
             0.,   0.,   0.,   0., 102., 170., 253., 253., 139.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   3., 128., 253., 253., 228.,
           179., 179., 179., 179., 245., 253., 253., 219.,  41.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 253., 253., 253.,
           253., 253., 253., 253., 253., 253., 253., 163.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 199., 253., 253.,
           253., 253., 253., 253., 253., 253., 170.,   9.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  36., 219., 253.,
           253., 253., 253., 253., 224.,  65.,  22.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:141

analyse the exceptions in iter:178
torch exception:
{'id': 4, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 128, 28, 28] to have 512 channels, but got 128 channels instead
mindspore exception:
{'id': 4, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 128, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 128, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:148

analyse the exceptions in iter:179
torch exception:
{'id': 8, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]],

         [[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]],

         [[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]],

         ...,

         [[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]],

         [[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]],

         [[-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          ...,
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338],
          [-6.4338, -6.4338, -6.4338,  ..., -6.4338, -6.4338, -6.4338]]]],
       grad_fn=<UnsafeViewBackward0>)]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 64, 28, 100] to have 512 channels, but got 64 channels instead
mindspore exception:
{'id': 8, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 64, 28, 100], dtype=Float32, value=
[[[[-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   ...
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001]],
  [[-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   ...
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001]],
  [[-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   ...
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001]],
  ...
  [[-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   ...
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001],
   [-2.64069481e+001, -2.64069481e+001, -2.64069481e+001 ... -2.64069405e+001, -2.64069405e+001, -2.64069405e+001]],
  [[-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   ...
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001]],
  [[-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   ...
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001],
   [-1.52414427e+001, -1.52414427e+001, -1.52414427e+001 ... -1.52414427e+001, -1.52414427e+001, -1.52414427e+001]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 64, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:149

analyse output arrays in iter:183

pre layer res:
5:transpose
{'name': 'transpose', 'output': array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ],
         [ 2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ,
           2.4662762 ,  2.4662762 ,  2.4662762 ,  2.4662762 ],
         [-3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ,
          -3.9626162 , -3.9626162 , -3.9626162 , -3.9626162 ],
         [-0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024,
          -0.31794024, -0.31794024, -0.31794024, -0.31794024],
         [ 1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ,
           1.3117061 ,  1.3117061 ,  1.3117061 ,  1.3117061 ],
         [ 0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ,
           0.5906683 ,  0.5906683 ,  0.5906683 ,  0.5906683 ],
         [ 2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ,
           2.4307203 ,  2.4307203 ,  2.4307203 ,  2.4307203 ],
         [ 1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ,
           1.1855769 ,  1.1855769 ,  1.1855769 ,  1.1855769 ],
         [-2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ,
          -2.0141492 , -2.0141492 , -2.0141492 , -2.0141492 ],
         [-3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ,
          -3.2415786 , -3.2415786 , -3.2415786 , -3.2415786 ],
         [ 1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ,
           1.0419149 ,  1.0419149 ,  1.0419149 ,  1.0419149 ],
         [-0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554,
          -0.29174554, -0.29174554, -0.29174554, -0.29174554],
         [-0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ],
         [-0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ],
         [ 0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ],
         [ 0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ,
           0.6282805 ,  0.6282805 ,  0.6282805 ,  0.6282805 ],
         [ 0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555],
         [-0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ,
          -0.4178747 , -0.4178747 , -0.4178747 , -0.4178747 ],
         [ 1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ,
           1.9285691 ,  1.9285691 ,  1.9285691 ,  1.9285691 ],
         [ 0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ,
           0.8095547 ,  0.8095547 ,  0.8095547 ,  0.8095547 ],
         [ 0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555,
           0.68342555,  0.68342555,  0.68342555,  0.68342555],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ,  0.        ]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [12], 'to': [4]}
tf node:
{'name': 'log', 'output': array([[[[       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [ 0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863,
           0.27132863,  0.27132863,  0.27132863,  0.27132863],
         [-0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064,
          -0.52650064, -0.52650064, -0.52650064, -0.52650064],
         [ 0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765],
         [ 0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951,
           0.17022951,  0.17022951,  0.17022951,  0.17022951],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031,
           0.04106031,  0.04106031,  0.04106031,  0.04106031],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [-0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094],
         [-0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853,
          -0.46476853, -0.46476853, -0.46476853, -0.46476853],
         [-0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834],
         [-0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094,
          -0.21127094, -0.21127094, -0.21127094, -0.21127094],
         [-0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756,
          -0.38063756, -0.38063756, -0.38063756, -0.38063756],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [5], 'to': [1]}
ms node:
{'name': 'log', 'output': array([[[[       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [ 0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ,
           0.9027101 ,  0.9027101 ,  0.9027101 ,  0.9027101 ],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125,
           0.27133125,  0.27133125,  0.27133125,  0.27133125],
         [-0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ,
          -0.5265023 , -0.5265023 , -0.5265023 , -0.5265023 ],
         [ 0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616,
           0.88818616,  0.88818616,  0.88818616,  0.88818616],
         [ 0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783,
           0.17022783,  0.17022783,  0.17022783,  0.17022783],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372,
           0.04106372,  0.04106372,  0.04106372,  0.04106372],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [-0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349],
         [-0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582,
          -0.46476582, -0.46476582, -0.46476582, -0.46476582],
         [-0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834],
         [-0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349,
          -0.21127349, -0.21127349, -0.21127349, -0.21127349],
         [-0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967,
          -0.38063967, -0.38063967, -0.38063967, -0.38063967],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf]]]],
      dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [5], 'to': [1]}
torch node:
{'name': 'log', 'output': array([[[[       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf],
         [ 0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936,
           0.90270936,  0.90270936,  0.90270936,  0.90270936],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845,
           0.27132845,  0.27132845,  0.27132845,  0.27132845],
         [-0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ,
          -0.5265008 , -0.5265008 , -0.5265008 , -0.5265008 ],
         [ 0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765,
           0.88818765,  0.88818765,  0.88818765,  0.88818765],
         [ 0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ,
           0.1702293 ,  0.1702293 ,  0.1702293 ,  0.1702293 ],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019,
           0.04106019,  0.04106019,  0.04106019,  0.04106019],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [-0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109],
         [-0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862,
          -0.46476862, -0.46476862, -0.46476862, -0.46476862],
         [-0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774],
         [        nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan,
                  nan,         nan,         nan,         nan],
         [ 0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834,
           0.65677834,  0.65677834,  0.65677834,  0.65677834],
         [-0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109,
          -0.21127109, -0.21127109, -0.21127109, -0.21127109],
         [-0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774,
          -0.38063774, -0.38063774, -0.38063774, -0.38063774],
         [       -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf,
                 -inf,        -inf,        -inf,        -inf]]]],
      dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [5], 'to': [1]}

generate models:152

analyse the exceptions in iter:203
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  97., 125., 125., 125., 125., 125., 125., 125., 125.,
             58.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  40.,
            226., 249., 253., 254., 254., 255., 254., 254., 255., 255., 254.,
            248.,  58.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  44., 159., 247.,
            254., 254., 254., 254., 254., 254., 254., 254., 254., 254., 254.,
            254., 248.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 126., 241., 254.,
            254., 254., 173., 149., 149., 127.,  19.,  36., 149., 157., 254.,
            254., 187.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  22., 136.,
            116.,  25.,   6.,   0.,   0.,   0.,   0.,  64.,  99., 178., 254.,
            201.,  89.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  65., 113., 243., 254., 254., 254.,
            117.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 162., 216., 243., 254., 254., 254., 248.,  96.,
             18.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             66., 117., 210., 243., 254., 254., 202., 176.,  95.,  42.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            173., 254., 254., 254., 254., 114.,  17.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            127., 189., 206., 254., 254., 191.,  22.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  17.,  95., 238., 254., 211.,  99.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  79., 239., 254., 228.,  21.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 177., 254., 249.,  80.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   9., 189., 254., 239.,  51.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  17., 161., 164.,  43.,   4.,   0.,   0.,
              0.,   0.,   0.,  39., 183., 254., 249., 126.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0., 138., 255., 254., 254.,  25.,   0.,  14.,
              0.,  22., 128., 180., 254., 250., 193.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0., 236., 254., 254., 254., 161., 151., 204.,
            151., 234., 255., 254., 250., 126.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 242., 254., 254., 254., 254., 254.,
            254., 251., 241., 143.,  88.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  55., 117., 117., 167., 248., 248.,
            207.,  84.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:169

analyse the exceptions in iter:204
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])]}
Given groups=1, weight of size [256, 256, 3, 3], expected input[1, 1, 14, 14] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 14, 14], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:170

analyse output arrays in iter:206

pre layer res:
10:log
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 14, 14]), 'from': [17], 'to': [4]}
tf node:
{'name': 'conv2d', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [10], 'to': [5]}
ms node:
{'name': 'conv2d', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': (1, 128, 14, 14), 'from': [10], 'to': [5]}
torch node:
{'name': 'conv2d', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 14, 14]), 'from': [10], 'to': [5]}

generate models:171

analyse the exceptions in iter:214
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  53.,
            101., 115., 255., 212., 194., 101., 101.,  98.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0., 130.,  47., 197., 200., 200., 200., 200., 228.,
            253., 253., 253., 253., 253., 253., 253., 247.,  27.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,  59., 231., 253., 253., 253., 253., 253., 253., 253.,
            253., 253., 253., 253., 253., 253., 253., 253., 207.,  26.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   3., 202., 253., 253., 239., 160., 160., 160., 160.,
            160., 160., 160., 160., 160., 160., 239., 253., 253., 215.,  37.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,  77., 214.,  65.,  52.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  99., 253., 253., 253.,  99.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  62., 253., 253., 253.,  99.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  62., 253., 253., 202.,   6.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  15., 179., 253., 253., 199.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 116., 253., 253., 253., 199.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  15., 241., 253., 253., 220.,  45.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  16., 253., 253., 253., 145.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  16., 253., 253., 253., 145.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 152., 253., 253., 251., 106.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   5., 175., 253., 253., 245.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  70., 253., 253., 253., 175.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 108., 253., 253., 238.,  69.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 224., 253., 253., 191.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 224., 253., 253., 191.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 224., 253., 253.,  65.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 129., 253., 182.,  15.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:178

analyse the exceptions in iter:228
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   3., 120., 202., 254., 254., 255.,
           160.,  79.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,  11., 253., 249., 246., 244., 243.,
           253., 253., 208.,  26.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   4.,  89.,  47.,  27.,   9.,   0.,
            85., 165., 252., 148.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   8., 250., 196.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            42., 181., 253., 219.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 103.,
           237., 253., 204.,  34.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 175., 253.,
           207.,  93.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  71., 252., 253.,
            52.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 235., 253.,
           244., 157.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16., 176.,
           231., 253., 231., 148.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            28., 163., 249., 253., 242., 121.,  35.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,  19., 135., 208., 253., 191.,  82.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,  10., 169., 253., 234., 108.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   5., 170., 253., 227.,  41.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   8., 179., 253., 168.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  94., 253., 219.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   1.,  13.,   0.,   0.,  12., 214., 237.,  35.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   5., 213., 189., 124.,  91., 206., 254., 138.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   1.,  96., 229., 253., 254., 253., 254., 146.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,  25., 121., 203., 253., 187.,  14.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [128, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:190

analyse the exceptions in iter:238
torch exception:
{'id': 5, 'name': 'linear', 'frame_work': 'torch', 'input_datas': [tensor([[[[ 64516.,      0.,      0.,  ..., 223729.,  64516.,      0.],
          [     0.,   7744., 191844.,  ...,  64516.,      0.,   7744.],
          [191844.,  64516., 259081.,  ...,   7744., 191844.,  64516.],
          ...,
          [ 64516.,  64516., 258064.,  ..., 173056., 258064.,  64516.],
          [258064., 258064.,  64516.,  ..., 258064., 258064., 258064.],
          [ 64516., 258064.,  64516.,  ...,  64516.,  64516., 258064.]],

         [[ 64516.,      0.,      0.,  ..., 223729.,  64516.,      0.],
          [     0.,   7744., 191844.,  ...,  64516.,      0.,   7744.],
          [191844.,  64516., 259081.,  ...,      0.,  47961.,  64516.],
          ...,
          [ 64516.,  64516., 258064.,  ...,  64516., 258064.,  64516.],
          [258064., 258064.,  64516.,  ...,  64516.,  64516.,  64516.],
          [ 64516., 258064.,  64516.,  ...,      0.,      0.,  64516.]],

         [[ 64516.,      0.,      0.,  ...,  47961.,      0.,      0.],
          [     0.,   7744., 191844.,  ...,  64516.,      0.,   7744.],
          [191844.,  64516., 259081.,  ...,      0.,  47961.,  64516.],
          ...,
          [ 64516.,  64516., 258064.,  ...,  64516., 258064.,  64516.],
          [258064., 258064.,  64516.,  ...,  64516.,  64516.,  64516.],
          [ 64516., 258064.,  64516.,  ...,      0.,      0.,  64516.]],

         ...,

         [[     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          ...,
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.]],

         [[     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          ...,
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.]],

         [[     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          ...,
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.],
          [     0.,      0.,      0.,  ...,      0.,      0.,      0.]]]],
       grad_fn=<ReshapeAliasBackward0>)]}
mat1 and mat2 shapes cannot be multiplied (1792x7 and 100x100)
mindspore exception:
{'id': 5, 'name': 'linear', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 7, 7], dtype=Float32, value=
[[[[6.45160000e+004, 0.00000000e+000, 0.00000000e+000 ... 2.23729000e+005, 6.45160000e+004, 0.00000000e+000],
   [0.00000000e+000, 7.74400000e+003, 1.91844000e+005 ... 6.45160000e+004, 0.00000000e+000, 7.74400000e+003],
   [1.91844000e+005, 6.45160000e+004, 2.59081000e+005 ... 7.74400000e+003, 1.91844000e+005, 6.45160000e+004],
   ...
   [6.45160000e+004, 6.45160000e+004, 2.58064000e+005 ... 1.73056000e+005, 2.58064000e+005, 6.45160000e+004],
   [2.58064000e+005, 2.58064000e+005, 6.45160000e+004 ... 2.58064000e+005, 2.58064000e+005, 2.58064000e+005],
   [6.45160000e+004, 2.58064000e+005, 6.45160000e+004 ... 6.45160000e+004, 6.45160000e+004, 2.58064000e+005]],
  [[6.45160000e+004, 0.00000000e+000, 0.00000000e+000 ... 2.23729000e+005, 6.45160000e+004, 0.00000000e+000],
   [0.00000000e+000, 7.74400000e+003, 1.91844000e+005 ... 6.45160000e+004, 0.00000000e+000, 7.74400000e+003],
   [1.91844000e+005, 6.45160000e+004, 2.59081000e+005 ... 0.00000000e+000, 4.79610000e+004, 6.45160000e+004],
   ...
   [6.45160000e+004, 6.45160000e+004, 2.58064000e+005 ... 6.45160000e+004, 2.58064000e+005, 6.45160000e+004],
   [2.58064000e+005, 2.58064000e+005, 6.45160000e+004 ... 6.45160000e+004, 6.45160000e+004, 6.45160000e+004],
   [6.45160000e+004, 2.58064000e+005, 6.45160000e+004 ... 0.00000000e+000, 0.00000000e+000, 6.45160000e+004]],
  [[6.45160000e+004, 0.00000000e+000, 0.00000000e+000 ... 4.79610000e+004, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 7.74400000e+003, 1.91844000e+005 ... 6.45160000e+004, 0.00000000e+000, 7.74400000e+003],
   [1.91844000e+005, 6.45160000e+004, 2.59081000e+005 ... 0.00000000e+000, 4.79610000e+004, 6.45160000e+004],
   ...
   [6.45160000e+004, 6.45160000e+004, 2.58064000e+005 ... 6.45160000e+004, 2.58064000e+005, 6.45160000e+004],
   [2.58064000e+005, 2.58064000e+005, 6.45160000e+004 ... 6.45160000e+004, 6.45160000e+004, 6.45160000e+004],
   [6.45160000e+004, 2.58064000e+005, 6.45160000e+004 ... 0.00000000e+000, 0.00000000e+000, 6.45160000e+004]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 7 and 'x2_row': 100. And 'x' shape [1792, 7](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:199

analyse output arrays in iter:239

pre layer res:
6:relu
{'name': 'relu', 'output': array([[[[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]],

        [[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]],

        [[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]],

        ...,

        [[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]],

        [[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]],

        [[2048., 3072., 3072., ..., 3072., 3072., 2048.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         ...,
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [3072., 4608., 4608., ..., 4608., 4608., 3072.],
         [2048., 3072., 3072., ..., 3072., 3072., 2048.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [5], 'to': [12]}
tf node:
{'name': 'cos', 'output': array([[[[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        ...,

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.88809365, -0.7541597 , -0.7541597 , ..., -0.7541597 ,
          -0.7541597 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [6], 'to': [10]}
ms node:
{'name': 'cos', 'output': array([[[[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        ...,

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]]]], dtype=float32), 'output_shape': (1, 128, 28, 28), 'from': [6], 'to': [10]}
torch node:
{'name': 'cos', 'output': array([[[[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        ...,

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]],

        [[ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         ...,
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.88809365, -0.7541596 , -0.7541596 , ..., -0.7541596 ,
          -0.7541596 ,  0.88809365],
         [ 0.94973433,  0.88809365,  0.88809365, ...,  0.88809365,
           0.88809365,  0.94973433]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 28, 28]), 'from': [6], 'to': [10]}

generate models:200

analyse output arrays in iter:244

pre layer res:
10:add
{'name': 'add', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [9, 13], 'to': [3]}
tf node:
{'name': 'conv2d', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [10], 'to': [4]}
ms node:
{'name': 'conv2d', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': (1, 64, 28, 28), 'from': [10], 'to': [4]}
torch node:
{'name': 'conv2d', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': torch.Size([1, 64, 28, 28]), 'from': [10], 'to': [4]}

generate models:204

analyse the exceptions in iter:258
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  60.,  77., 105., 221., 112.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  36., 155., 242.,
            242., 242., 242., 242., 252., 254., 254., 254., 112.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 113., 254., 238.,
            205., 205., 174., 205., 167., 116., 208., 254., 112.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 209., 211.,
              2.,   0.,   0.,   0.,   0.,   0., 170., 254.,  99.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 142.,
              3.,   0.,   0.,   0.,   0.,   1., 178., 249.,  20.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  40., 254., 152.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 125., 254.,  98.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  12., 229., 232.,   8.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 153., 254., 153.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   6., 207., 254.,  70.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  75., 254., 207.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 190., 254., 123.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  17., 227., 248.,  46.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  74., 254., 249.,  49.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  16., 198., 254., 181.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 140., 254., 248.,  53.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             51., 234., 254., 101.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            171., 254., 177.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  65.,
            254., 254.,  84.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26.,
            252., 182.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 12x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 12. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 12](transpose_b=True).

generate models:217

analyse the exceptions in iter:262
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  51.,  79., 104.,  44.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 103., 252., 224., 119.,  26.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  19., 168., 205., 243., 210.,  28.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  13., 175., 253., 133.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  10., 179., 255., 197.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 110., 252., 253., 121.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  23., 197., 240., 252., 244.,  56.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 128., 252., 252., 252.,  75.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  19.,
            154., 253., 254., 253., 194.,  63.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 169.,
            252., 252., 222., 121.,  19.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 198., 234.,
            252., 214.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51., 253., 227.,
            103.,  15.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 253., 214.,  38.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  38., 209., 252.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  10., 110., 252.,  77.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   4., 166., 177.,   3.,   0.,   0.,
              0.,   0.,  51.,   0.,   0.,  51.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 170., 216.,  28.,   4.,  10.,
             48., 241.,  41.,  60.,  10., 117., 192.,  84.,  51.,  38.,  13.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  82., 240., 215.,  79., 122.,
             97., 252., 159., 178., 197., 215., 166., 196., 206., 243., 194.,
             44.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 159., 252., 253., 252.,
            177., 252., 223., 177., 168., 168., 216., 214., 214., 240., 253.,
            240.,  81.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,  28., 153., 177.,
            139., 139.,  28.,   9.,   0.,   0.,  16.,  15.,  28.,  78., 153.,
            252., 118.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 3, 3], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:220

analyse output arrays in iter:266

pre layer res:
22:cos
{'name': 'cos', 'output': array([[[[ 1.        ,  1.        ,  1.        ,  1.        ,
           1.        ,  1.        ,  1.        ],
         [ 1.        , -0.11038724,  0.78221214, -0.1016157 ,
           0.78221214, -0.1016157 ,  1.        ],
         [ 1.        ,  0.96945935,  0.78221214, -0.9952637 ,
           0.91474235,  1.        ,  1.        ],
         [ 1.        ,  1.        , -0.47162572, -0.8623036 ,
          -0.3755375 ,  1.        ,  1.        ],
         [ 1.        , -0.81928825, -0.9952637 , -0.9793607 ,
           0.78221214,  0.5984842 ,  1.        ],
         [ 1.        ,  0.5025704 ,  0.78221214, -0.1016157 ,
           0.78221214,  0.78221214,  1.        ],
         [ 1.        ,  1.        ,  1.        ,  1.        ,
           1.        ,  1.        ,  1.        ]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 7, 7]), 'from': [2], 'to': [24]}
tf node:
{'name': 'log', 'output': array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ],
         [ 0.        ,         nan, -0.2456293 ,         nan,
          -0.2456293 ,         nan,  0.        ],
         [ 0.        , -0.03101673, -0.2456293 ,         nan,
          -0.08911284,  0.        ,  0.        ],
         [ 0.        ,  0.        ,         nan,         nan,
                  nan,  0.        ,  0.        ],
         [ 0.        ,         nan,         nan,         nan,
          -0.2456293 , -0.51335514,  0.        ],
         [ 0.        , -0.6880196 , -0.2456293 ,         nan,
          -0.2456293 , -0.2456293 ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 7, 7]), 'from': [22], 'to': [3]}
ms node:
{'name': 'log', 'output': array([[[[-1.4305115e-06, -1.4305115e-06, -1.4305115e-06,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06,
          -1.4305115e-06],
         [-1.4305115e-06,            nan, -2.4563171e-01,
                     nan, -2.4563171e-01,            nan,
          -1.4305115e-06],
         [-1.4305115e-06, -3.1018158e-02, -2.4563171e-01,
                     nan, -8.9113824e-02, -1.4305115e-06,
          -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06,            nan,
                     nan,            nan, -1.4305115e-06,
          -1.4305115e-06],
         [-1.4305115e-06,            nan,            nan,
                     nan, -2.4563171e-01, -5.1335663e-01,
          -1.4305115e-06],
         [-1.4305115e-06, -6.8802100e-01, -2.4563171e-01,
                     nan, -2.4563171e-01, -2.4563171e-01,
          -1.4305115e-06],
         [-1.4305115e-06, -1.4305115e-06, -1.4305115e-06,
          -1.4305115e-06, -1.4305115e-06, -1.4305115e-06,
          -1.4305115e-06]]]], dtype=float32), 'output_shape': (1, 1, 7, 7), 'from': [22], 'to': [3]}
torch node:
{'name': 'log', 'output': array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ],
         [ 0.        ,         nan, -0.24562937,         nan,
          -0.24562937,         nan,  0.        ],
         [ 0.        , -0.03101673, -0.24562937,         nan,
          -0.08911284,  0.        ,  0.        ],
         [ 0.        ,  0.        ,         nan,         nan,
                  nan,  0.        ,  0.        ],
         [ 0.        ,         nan,         nan,         nan,
          -0.24562937, -0.51335514,  0.        ],
         [ 0.        , -0.6880196 , -0.24562937,         nan,
          -0.24562937, -0.24562937,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,
           0.        ,  0.        ,  0.        ]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 7, 7]), 'from': [22], 'to': [3]}

generate models:224

analyse output arrays in iter:269

pre layer res:
14:sin
{'name': 'sin', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [1], 'to': [2]}
tf node:
{'name': 'linear', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 100]), 'from': [14], 'to': [15]}
ms node:
{'name': 'linear', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': (1, 256, 28, 100), 'from': [14], 'to': [15]}
torch node:
{'name': 'linear', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 100]), 'from': [14], 'to': [15]}

generate models:227

analyse the exceptions in iter:272
torch exception:
{'id': 15, 'name': 'flatten', 'frame_work': 'torch', 'input_datas': [tensor([159.4352])]}
Dimension out of range (expected to be in range of [-1, 0], but got 1)

generate models:229

analyse output arrays in iter:274

pre layer res:
7:add
{'name': 'add', 'output': array([[[[8.41471e-03, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00],
         [0.00000e+00, 9.00000e+00, 7.60000e+01, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00],
         [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00],
         ...,
         [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00],
         [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00],
         [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,
          0.00000e+00, 0.00000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 14, 100]), 'from': [10, 15], 'to': [4]}
tf node:
{'name': 'conv2d', 'output': array([[[[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[8.4147099e-03, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [4.7683716e-06, 9.0000420e+00, 7.6000000e+01, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 4.7683716e-06, 6.9287939e-06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 100]), 'from': [7], 'to': []}
ms node:
{'name': 'conv2d', 'output': array([[[[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        ...,

        [[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414708e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]]]], dtype=float32), 'output_shape': (1, 256, 14, 100), 'from': [7], 'to': []}
torch node:
{'name': 'conv2d', 'output': array([[[[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        ...,

        [[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]],

        [[8.414709e-03, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 9.000000e+00, 7.600000e+01, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         ...,
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00],
         [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,
          0.000000e+00, 0.000000e+00]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 100]), 'from': [7], 'to': []}

generate models:231

analyse the exceptions in iter:275
torch exception:
{'id': 1, 'name': 'linear', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<TransposeBackward0>)]}
mat1 and mat2 shapes cannot be multiplied (14336x28 and 100x100)
mindspore exception:
{'id': 1, 'name': 'linear', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [14336, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:232

analyse the exceptions in iter:276
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])]}
Given groups=1, weight of size [512, 256, 1, 1], expected input[1, 1, 28, 100] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:233

analyse output arrays in iter:282

pre layer res:
3:exp
{'name': 'exp', 'output': array([[[[1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          5.9874145e+04,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf, 4.6071865e+28, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 3.2690175e+06,           inf,           inf,
                    inf, 1.6948892e+28, 8.8861110e+06, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 8.8861110e+06,
          4.6071865e+28,           inf,           inf, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 8.4383568e+26,           inf,           inf,
          1.4841316e+02, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 3.4042762e+29, 1.4462571e+12, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0686474e+13,           inf,           inf,
                    inf, 8.1030840e+03, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.7392750e+18,           inf,
                    inf,           inf, 6.0760303e+37, 5.8346171e+14,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 8.1030840e+03,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 1.0686474e+13, 5.3204822e+11,
          2.0085537e+01, 1.0000000e+00, 1.0000000e+00, 1.3188157e+09,
          1.0966332e+03, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0966332e+03,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
          3.1042978e+26, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 3.5849126e+09, 2.5154387e+30,           inf,
                    inf,           inf,           inf,           inf,
          3.1042978e+26, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
          3.1042978e+26, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
          3.1042978e+26, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
          7.3890562e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 3.1855931e+16,           inf,           inf,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf, 1.2523633e+29,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 8.4383568e+26,           inf, 3.8310077e+22,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.2851600e+19,           inf, 3.8310077e+22,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.2851600e+19,           inf, 2.2352465e+37,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.2026044e+06,           inf,           inf,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [0], 'to': [8]}
tf node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [3], 'to': [4]}
ms node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan, 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ]]]], dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [3], 'to': [4]}
torch node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [3], 'to': [4]}

generate models:238

analyse the exceptions in iter:285
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23., 107., 107.,
           156., 167.,  97.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 207., 253., 253.,
           253., 253., 252.,  93.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   2., 163., 253., 247., 188.,
           112., 225., 253., 235.,  43.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,  93., 253., 245.,  98.,   0.,
             0.,   3., 183., 253.,  76.,   0.,   0.,   0.,   2., 111., 160.,
           160., 160., 160., 105.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 164., 253., 200.,   0.,   0.,
             0.,   0.,  60., 253., 159.,   0.,  50.,  60., 124., 253., 253.,
           253., 253., 253., 253.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 254., 243.,  85.,   0.,   0.,
             0.,   0.,  24., 253., 194.,  96., 244., 253., 253., 253., 239.,
           176., 176., 176., 122.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 255., 231.,  31.,   0.,   0.,
             0.,   0., 124., 253., 244., 244., 253., 253., 199.,  41.,  33.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 254., 253., 158., 113.,  21.,
             0.,   3., 191., 253., 253., 253., 211., 141.,  14.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 132., 241., 241., 250., 245.,
           131., 243., 252., 241., 214.,  88.,   3.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79.,  69.,
             0., 102.,  96.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:241

analyse the exceptions in iter:287
torch exception:
{'id': 2, 'name': 'linear', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ReshapeAliasBackward0>)]}
mat1 and mat2 shapes cannot be multiplied (7168x28 and 100x100)
mindspore exception:
{'id': 2, 'name': 'linear', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [7168, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:243

analyse the exceptions in iter:295
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  41., 137., 137., 137., 253., 242.,  60.,  14.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  98., 225., 254., 254., 254., 254., 254., 254., 189.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 160., 254., 213., 145., 201., 195.,  83., 195., 233.,
             36.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 160., 177.,  15.,   0.,   0.,   0.,   0., 166., 224.,
             28.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  30.,  14.,   0.,   0.,   0.,   0.,   0., 166., 254.,
             53.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              3., 108.,  40.,   0.,   0.,   0.,   0.,   6., 122., 245., 196.,
              6.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              6., 254.,  95.,   0.,   0.,   0.,   3., 128., 254., 251., 129.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              6., 254., 164.,   8.,   0.,  65., 192., 254., 241.,  80.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              3., 184., 254., 133., 160., 240., 254., 227.,  40.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  60., 254., 255., 254., 255., 147.,  38.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              2.,  95., 254., 254., 254.,  90.,   6.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  41.,
            154., 254., 254., 255., 254.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 160.,
            252., 141.,  87., 221., 254.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 206., 254.,
            182.,   0.,   0., 178., 254., 102.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 148., 254., 178.,
              5.,   0.,   0., 178., 254., 159.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  54., 254., 238.,  24.,
              0.,   0.,  28., 228., 254.,  82.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  54., 254., 165.,   0.,
              0.,   0., 128., 254., 254.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  54., 254., 236.,  62.,
             44., 202., 235., 254., 154.,   9.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 199., 254., 240.,
            237., 254., 254., 203.,   8.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  46., 233., 254.,
            254., 254., 179.,  49.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:250

analyse output arrays in iter:299

pre layer res:
7:maxpool2d
{'name': 'maxpool2d', 'output': array([[[[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]],

        [[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]],

        [[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]],

        ...,

        [[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]],

        [[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]],

        [[5731726. , 5731726. , 5731726. , ..., 5731726. , 5731726. ,
          5731726. ],
         [5119461.5, 5119461.5, 5119461.5, ..., 5119461.5, 5119461.5,
          5119461.5],
         [4826995. , 4826995. , 4826995. , ..., 4826995. , 4826995. ,
          4826995. ],
         ...,
         [4835155. , 4835155. , 4835155. , ..., 4835155. , 4835155. ,
          4835155. ],
         [6705514.5, 6705514.5, 6705514.5, ..., 6705514.5, 6705514.5,
          6705514.5],
         [7248128. , 7248128. , 7248128. , ..., 7248128. , 7248128. ,
          7248128. ]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 7, 50]), 'from': [6], 'to': [14]}
tf node:
{'name': 'sin', 'output': array([[[[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        ...,

        [[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.15859582, -0.15859582, -0.15859582, ..., -0.15859582,
          -0.15859582, -0.15859582],
         [-0.60640085, -0.60640085, -0.60640085, ..., -0.60640085,
          -0.60640085, -0.60640085],
         [ 0.6590932 ,  0.6590932 ,  0.6590932 , ...,  0.6590932 ,
           0.6590932 ,  0.6590932 ],
         ...,
         [-0.9078391 , -0.9078391 , -0.9078391 , ..., -0.9078391 ,
          -0.9078391 , -0.9078391 ],
         [-0.983842  , -0.983842  , -0.983842  , ..., -0.983842  ,
          -0.983842  , -0.983842  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 7, 50]), 'from': [7], 'to': [8]}
ms node:
{'name': 'sin', 'output': array([[[[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        ...,

        [[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.76823723, -0.76823723, -0.76823723, ..., -0.76823723,
          -0.76823723, -0.76823723],
         [ 0.99814844,  0.99814844,  0.99814844, ...,  0.99814844,
           0.9051187 ,  0.9051187 ],
         [-0.53421   , -0.53421   , -0.53421   , ..., -0.53421   ,
          -0.53421   , -0.53421   ],
         ...,
         [-0.99773586, -0.99773586, -0.99773586, ..., -0.99773586,
          -0.99773586, -0.99773586],
         [ 0.24662325,  0.24662325,  0.24662325, ...,  0.24662325,
           0.24662325,  0.24662325],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]]]], dtype=float32), 'output_shape': (1, 512, 7, 50), 'from': [7], 'to': [8]}
torch node:
{'name': 'sin', 'output': array([[[[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        ...,

        [[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]],

        [[-0.9997558 , -0.9997558 , -0.9997558 , ..., -0.9997558 ,
          -0.88796335, -0.88796335],
         [ 0.8311734 ,  0.8311734 ,  0.8311734 , ...,  0.8311734 ,
           0.46285638,  0.46285638],
         [-0.9982513 , -0.9982513 , -0.9982513 , ..., -0.9982513 ,
          -0.9043881 , -0.9043881 ],
         ...,
         [-0.9800573 , -0.9800573 , -0.9800573 , ..., -0.9800573 ,
          -0.9800573 , -0.9800573 ],
         [-0.682228  , -0.682228  , -0.682228  , ..., -0.682228  ,
          -0.682228  , -0.682228  ],
         [ 0.5910197 ,  0.5910197 ,  0.5910197 , ...,  0.5910197 ,
           0.5910197 ,  0.5910197 ]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 7, 50]), 'from': [7], 'to': [8]}

generate models:254

analyse output arrays in iter:301

pre layer res:
13:add
{'name': 'add', 'output': array([[[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           2.49000000e+02,  2.55000000e+02,  4.80000000e+01,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  2.18366547e+01,
           8.43796082e+01,  5.64506149e+00,  1.66063538e+02,
           2.53395920e+02,  2.53000000e+02,  2.37000000e+02,
           2.34000000e+02,  2.34000000e+02,  1.59000000e+02,
           2.34000000e+02,  2.34000000e+02,  1.15000000e+02,
           7.30000000e+01,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00, -4.28182662e-01, -5.21551013e-01,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.77331924e+01,
           2.48184753e+02,  2.15994827e+02,  2.53994827e+02,
           2.53784958e+02,  1.84038605e+02,  1.77000000e+02,
           2.33000000e+02,  2.53000000e+02,  2.53000000e+02,
           2.53000000e+02,  2.53000000e+02,  2.53000000e+02,
           1.91000000e+02,  0.00000000e+00,  0.00000000e+00,
           1.23573124e-01,  5.30534871e-02,  9.94823754e-01,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  1.91720581e+02,
           2.53980240e+02,  2.53994827e+02,  2.53994827e+02,
           2.28955780e+02,  2.07376251e+01,  0.00000000e+00,
           4.50000000e+01,  6.10000000e+01,  6.10000000e+01,
           1.55000000e+02,  2.53000000e+02,  2.53000000e+02,
           1.61000000e+02,  4.36164767e-01,  2.02149883e-01,
           5.30534871e-02,  9.94823754e-01,  9.94823754e-01,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  1.73524445e+02,
           2.53994827e+02,  2.53994827e+02,  2.53994827e+02,
           4.90028267e+01,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
           2.12000000e+02,  2.53000000e+02,  2.42000000e+02,
           4.64412117e+01, -9.81957853e-01,  9.94823754e-01,
           9.94823754e-01,  9.94823754e-01, -2.10781068e-01,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00, -7.27163196e-01,  5.39948235e+01,
           2.41994827e+02,  2.45329956e+02,  1.67737625e+02,
           3.14111996e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  1.70000000e+01,  1.54000000e+02,
           2.53000000e+02,  2.53141113e+02,  8.98775864e+01,
          -9.70528007e-01,  9.94823754e-01,  9.94823754e-01,
           5.80586672e-01,  9.92872655e-01,  1.41120002e-01,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00, -5.06391644e-01,  9.94823754e-01,
           1.73466206e+01,  5.08366547e+01,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  9.90000000e+01,  2.53000000e+02,
           2.53636734e+02,  1.99002823e+02,  1.79948235e+01,
           9.94823754e-01, -8.01134646e-01, -9.30105984e-01,
          -8.85130931e-03,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00, -7.68254697e-01, -9.81957853e-01,
           8.77589822e-01,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           2.10000000e+01,  2.03436157e+02,  2.53346649e+02,
           1.98018036e+02,  1.19948235e+01,  9.94823754e-01,
          -4.75523680e-01, -1.32351756e-01,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.98816669e-01,
           4.98739302e-01,  8.50903511e-01,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  8.36655617e-01,
           1.71976593e+02,  2.53784958e+02,  2.53994827e+02,
           1.29994827e+02,  6.23012185e-01, -9.42514479e-01,
          -7.56802499e-01,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.98816669e-01,
           9.94823754e-01, -9.66117799e-01,  0.00000000e+00,
          -9.61397469e-01, -9.99206841e-01,  5.79333115e+01,
           2.41994827e+02,  2.53994827e+02,  2.10498734e+02,
           2.38065567e+01, -4.91021603e-01,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.39519763e-01,
           9.94823754e-01, -9.66117799e-01, -9.58924294e-01,
          -6.19203374e-02,  9.94823754e-01,  1.70994827e+02,
           2.53994827e+02,  2.33467712e+02,  2.83363667e+01,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.98816669e-01,
           9.94823754e-01, -8.73311996e-01, -9.98347104e-01,
           9.94823754e-01,  5.99948235e+01,  2.36118195e+02,
           2.52806534e+02,  2.25094421e+02,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.98816669e-01,
           9.94823754e-01,  9.94823754e-01,  9.94823754e-01,
           3.99482369e+00,  1.67126709e+02,  2.52000015e+02,
           2.52000000e+02,  1.44000000e+02,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  9.45435345e-01,
           9.94823754e-01,  9.94823754e-01, -9.72119048e-02,
           1.77860077e+02,  2.52038605e+02,  2.53000000e+02,
           1.37000000e+02,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00, -6.76771998e-01,
           5.94908595e-01, -7.02407777e-01,  5.41235733e+01,
           2.50000000e+02,  2.53000000e+02,  1.88000000e+02,
           4.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  5.70000000e+01,  2.37000000e+02,
           2.53000000e+02,  1.75000000e+02,  2.50000000e+01,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  1.51000000e+02,  2.53000000e+02,
           2.53000000e+02,  1.30000000e+02,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           4.70000000e+01,  2.23000000e+02,  2.53000000e+02,
           2.35000000e+02,  2.20000000e+01,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  3.50000000e+01,
           2.23000000e+02,  2.53000000e+02,  2.53000000e+02,
           5.80000000e+01,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  5.60000000e+01,
           2.53000000e+02,  2.53000000e+02,  1.82000000e+02,
           3.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
           0.00000000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [2, 16], 'to': [4]}
tf node:
{'name': 'log', 'output': array([[[[          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           5.5174527e+00,  5.5412636e+00,  3.8712010e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.0835900e+00,
           4.4353256e+00,  1.7307811e+00,  5.1123705e+00,
           5.5349531e+00,  5.5333896e+00,  5.4680600e+00,
           5.4553213e+00,  5.4553213e+00,  5.0689044e+00,
           5.4553213e+00,  5.4553213e+00,  4.7449322e+00,
           4.2904596e+00,           -inf,           -inf,
                    -inf,            nan,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.5822411e+00,
           5.5141735e+00,  5.3752546e+00,  5.5373139e+00,
           5.5364871e+00,  5.2151456e+00,  5.1761498e+00,
           5.4510384e+00,  5.5333896e+00,  5.5333896e+00,
           5.5333896e+00,  5.5333896e+00,  5.5333896e+00,
           5.2522736e+00,           -inf,           -inf,
          -2.0909221e+00, -2.9364548e+00, -5.1896893e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.2560391e+00,
           5.5372567e+00,  5.5373139e+00,  5.5373139e+00,
           5.4335289e+00,  3.0319498e+00,           -inf,
           3.8066626e+00,  4.1108737e+00,  4.1108737e+00,
           5.0434251e+00,  5.5333896e+00,  5.5333896e+00,
           5.0814042e+00, -8.2973522e-01, -1.5987458e+00,
          -2.9364548e+00, -5.1896893e-03, -5.1896893e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.1563187e+00,
           5.5373139e+00,  5.5373139e+00,  5.5373139e+00,
           3.8918781e+00,           -inf,           -inf,
                    -inf,           -inf,  1.6094379e+00,
           5.3565865e+00,  5.5333896e+00,  5.4889379e+00,
           3.8381872e+00,            nan, -5.1896893e-03,
          -5.1896893e-03, -5.1896893e-03,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,  3.9888883e+00,
           5.4889164e+00,  5.5026040e+00,  5.1224012e+00,
           1.1445794e+00,           -inf,           -inf,
                    -inf,  2.8332133e+00,  5.0369525e+00,
           5.5333896e+00,  5.5339470e+00,  4.4984484e+00,
                     nan, -5.1896893e-03, -5.1896893e-03,
          -5.4371619e-01, -7.1528656e-03, -1.9581447e+00,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan, -5.1896893e-03,
           2.8533976e+00,  3.9286177e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.5951200e+00,  5.5333896e+00,
           5.5359030e+00,  5.2933192e+00,  2.8900840e+00,
          -5.1896893e-03,            nan,            nan,
                     nan,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,            nan,
          -1.3057597e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.0445225e+00,  5.3153524e+00,  5.5347586e+00,
           5.2883582e+00,  2.4844751e+00, -5.1896893e-03,
                     nan,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -6.9567174e-01, -1.6145654e-01,           -inf,
                    -inf,           -inf, -1.7834274e-01,
           5.1473584e+00,  5.5364871e+00,  5.5373139e+00,
           4.8674946e+00, -4.7318920e-01,            nan,
                     nan,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1896893e-03,            nan,           -inf,
                     nan,            nan,  4.0592928e+00,
           5.4889164e+00,  5.5373139e+00,  5.3494797e+00,
           3.1699610e+00,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.2386423e-02,
          -5.1896893e-03,            nan,            nan,
                     nan, -5.1896893e-03,  5.1416335e+00,
           5.5373139e+00,  5.4530439e+00,  3.3441460e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1896893e-03,            nan,            nan,
          -5.1896893e-03,  4.0942583e+00,  5.4643326e+00,
           5.5326247e+00,  5.4165201e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1896893e-03, -5.1896893e-03, -5.1896893e-03,
           1.3849994e+00,  5.1187525e+00,  5.5294290e+00,
           5.5294290e+00,  4.9698133e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.6109775e-02,
          -5.1896893e-03, -5.1896893e-03,            nan,
           5.1809974e+00,  5.5295825e+00,  5.5333896e+00,
           4.9199810e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,            nan,
          -5.1934749e-01,            nan,  3.9912698e+00,
           5.5214610e+00,  5.5333896e+00,  5.2364421e+00,
           1.3862944e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.0430512e+00,  5.4680600e+00,
           5.5333896e+00,  5.1647859e+00,  3.2188759e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  5.0172796e+00,  5.5333896e+00,
           5.5333896e+00,  4.8675346e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.8501477e+00,  5.4071717e+00,  5.5333896e+00,
           5.4595857e+00,  3.0910425e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.5553482e+00,
           5.4071717e+00,  5.5333896e+00,  5.5333896e+00,
           4.0604429e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.0253515e+00,
           5.5333896e+00,  5.5333896e+00,  5.2040067e+00,
           1.0986123e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [13], 'to': [1]}
ms node:
{'name': 'log', 'output': array([[[[          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           5.5174513e+00,  5.5412621e+00,  3.8711989e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.0835881e+00,
           4.4353237e+00,  1.7307839e+00,  5.1123734e+00,
           5.5349517e+00,  5.5333881e+00,  5.4680600e+00,
           5.4553204e+00,  5.4553204e+00,  5.0689049e+00,
           5.4553204e+00,  5.4553204e+00,  4.7449312e+00,
           4.2904563e+00,           -inf,           -inf,
                    -inf,            nan,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.5822392e+00,
           5.5141721e+00,  5.3752513e+00,  5.5373125e+00,
           5.5364857e+00,  5.2151456e+00,  5.1761527e+00,
           5.4510374e+00,  5.5333881e+00,  5.5333881e+00,
           5.5333881e+00,  5.5333881e+00,  5.5333881e+00,
           5.2522736e+00,           -inf,           -inf,
          -2.0909235e+00, -2.9364579e+00, -5.1911199e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.2560391e+00,
           5.5372553e+00,  5.5373125e+00,  5.5373125e+00,
           5.4335279e+00,  3.0319526e+00,           -inf,
           3.8066654e+00,  4.1108737e+00,  4.1108737e+00,
           5.0434237e+00,  5.5333881e+00,  5.5333881e+00,
           5.0814071e+00, -8.2973617e-01, -1.5987483e+00,
          -2.9364579e+00, -5.1911199e-03, -5.1911199e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.1563168e+00,
           5.5373125e+00,  5.5373125e+00,  5.5373125e+00,
           3.8918757e+00,           -inf,           -inf,
                    -inf,           -inf,  1.6094407e+00,
           5.3565831e+00,  5.5333881e+00,  5.4889379e+00,
           3.8381872e+00,            nan, -5.1911199e-03,
          -5.1911199e-03, -5.1911199e-03,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,  3.9888849e+00,
           5.4889164e+00,  5.5026040e+00,  5.1224036e+00,
           1.1445770e+00,           -inf,           -inf,
                    -inf,  2.8332105e+00,  5.0369511e+00,
           5.5333881e+00,  5.5339456e+00,  4.4984512e+00,
                     nan, -5.1911199e-03, -5.1911199e-03,
          -5.4371774e-01, -7.1542962e-03, -1.9581479e+00,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan, -5.1911199e-03,
           2.8533947e+00,  3.9286153e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.5951176e+00,  5.5333881e+00,
           5.5359015e+00,  5.2933168e+00,  2.8900847e+00,
          -5.1911199e-03,            nan,            nan,
                     nan,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,            nan,
          -1.3057703e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.0445206e+00,  5.3153501e+00,  5.5347571e+00,
           5.2883558e+00,  2.4844751e+00, -5.1911199e-03,
                     nan,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1854622e-03,
          -6.9567323e-01, -1.6145982e-01,           -inf,
                    -inf,           -inf, -1.7834602e-01,
           5.1473565e+00,  5.5364857e+00,  5.5373125e+00,
           4.8674932e+00, -4.7318840e-01,            nan,
                     nan,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1854622e-03,
          -5.1911199e-03,            nan,           -inf,
                     nan,            nan,  4.0592914e+00,
           5.4889164e+00,  5.5373125e+00,  5.3494773e+00,
           3.1699610e+00,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.2386464e-02,
          -5.1911199e-03,            nan,            nan,
                     nan, -5.1911199e-03,  5.1416316e+00,
           5.5373125e+00,  5.4530430e+00,  3.3441451e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1854622e-03,
          -5.1911199e-03,            nan,            nan,
          -5.1911199e-03,  4.0942583e+00,  5.4643326e+00,
           5.5326233e+00,  5.4165192e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1854622e-03,
          -5.1911199e-03, -5.1911199e-03, -5.1911199e-03,
           1.3849980e+00,  5.1187549e+00,  5.5294275e+00,
           5.5294275e+00,  4.9698100e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.6109753e-02,
          -5.1911199e-03, -5.1911199e-03,            nan,
           5.1810002e+00,  5.5295811e+00,  5.5333881e+00,
           4.9199781e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,            nan,
          -5.1934910e-01,            nan,  3.9912665e+00,
           5.5214596e+00,  5.5333881e+00,  5.2364421e+00,
           1.3862929e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.0430503e+00,  5.4680600e+00,
           5.5333881e+00,  5.1647840e+00,  3.2188735e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  5.0172782e+00,  5.5333881e+00,
           5.5333881e+00,  4.8675332e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.8501475e+00,  5.4071708e+00,  5.5333881e+00,
           5.4595847e+00,  3.0910454e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.5553486e+00,
           5.4071708e+00,  5.5333881e+00,  5.5333881e+00,
           4.0604420e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.0253506e+00,
           5.5333881e+00,  5.5333881e+00,  5.2040095e+00,
           1.0986100e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf]]]], dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [13], 'to': [1]}
torch node:
{'name': 'log', 'output': array([[[[          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           5.5174527e+00,  5.5412636e+00,  3.8712010e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.0835900e+00,
           4.4353256e+00,  1.7307811e+00,  5.1123705e+00,
           5.5349531e+00,  5.5333896e+00,  5.4680600e+00,
           5.4553213e+00,  5.4553213e+00,  5.0689044e+00,
           5.4553213e+00,  5.4553213e+00,  4.7449322e+00,
           4.2904596e+00,           -inf,           -inf,
                    -inf,            nan,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.5822411e+00,
           5.5141735e+00,  5.3752546e+00,  5.5373139e+00,
           5.5364871e+00,  5.2151456e+00,  5.1761498e+00,
           5.4510384e+00,  5.5333896e+00,  5.5333896e+00,
           5.5333896e+00,  5.5333896e+00,  5.5333896e+00,
           5.2522736e+00,           -inf,           -inf,
          -2.0909221e+00, -2.9364548e+00, -5.1897494e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.2560391e+00,
           5.5372562e+00,  5.5373139e+00,  5.5373139e+00,
           5.4335289e+00,  3.0319498e+00,           -inf,
           3.8066626e+00,  4.1108737e+00,  4.1108737e+00,
           5.0434251e+00,  5.5333896e+00,  5.5333896e+00,
           5.0814042e+00, -8.2973522e-01, -1.5987458e+00,
          -2.9364548e+00, -5.1897494e-03, -5.1897494e-03,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  5.1563187e+00,
           5.5373139e+00,  5.5373139e+00,  5.5373139e+00,
           3.8918781e+00,           -inf,           -inf,
                    -inf,           -inf,  1.6094379e+00,
           5.3565865e+00,  5.5333896e+00,  5.4889379e+00,
           3.8381872e+00,            nan, -5.1897494e-03,
          -5.1897494e-03, -5.1897494e-03,            nan,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,  3.9888883e+00,
           5.4889164e+00,  5.5026040e+00,  5.1224012e+00,
           1.1445794e+00,           -inf,           -inf,
                    -inf,  2.8332133e+00,  5.0369525e+00,
           5.5333896e+00,  5.5339470e+00,  4.4984484e+00,
                     nan, -5.1897494e-03, -5.1897494e-03,
          -5.4371619e-01, -7.1528656e-03, -1.9581447e+00,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan, -5.1897494e-03,
           2.8533976e+00,  3.9286177e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.5951200e+00,  5.5333896e+00,
           5.5359030e+00,  5.2933187e+00,  2.8900840e+00,
          -5.1897494e-03,            nan,            nan,
                     nan,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,            nan,            nan,
          -1.3057603e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.0445225e+00,  5.3153524e+00,  5.5347586e+00,
           5.2883582e+00,  2.4844751e+00, -5.1897494e-03,
                     nan,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -6.9567180e-01, -1.6145654e-01,           -inf,
                    -inf,           -inf, -1.7834274e-01,
           5.1473584e+00,  5.5364871e+00,  5.5373139e+00,
           4.8674946e+00, -4.7318920e-01,            nan,
                     nan,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1897494e-03,            nan,           -inf,
                     nan,            nan,  4.0592923e+00,
           5.4889164e+00,  5.5373139e+00,  5.3494797e+00,
           3.1699610e+00,            nan,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.2386423e-02,
          -5.1897494e-03,            nan,            nan,
                     nan, -5.1897494e-03,  5.1416335e+00,
           5.5373139e+00,  5.4530439e+00,  3.3441460e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1897494e-03,            nan,            nan,
          -5.1897494e-03,  4.0942583e+00,  5.4643326e+00,
           5.5326247e+00,  5.4165201e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.1840317e-03,
          -5.1897494e-03, -5.1897494e-03, -5.1897494e-03,
           1.3849994e+00,  5.1187525e+00,  5.5294290e+00,
           5.5294290e+00,  4.9698133e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.6109775e-02,
          -5.1897494e-03, -5.1897494e-03,            nan,
           5.1809969e+00,  5.5295825e+00,  5.5333896e+00,
           4.9199810e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,            nan,
          -5.1934761e-01,            nan,  3.9912698e+00,
           5.5214610e+00,  5.5333896e+00,  5.2364421e+00,
           1.3862944e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  4.0430512e+00,  5.4680600e+00,
           5.5333896e+00,  5.1647859e+00,  3.2188759e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,  5.0172796e+00,  5.5333896e+00,
           5.5333896e+00,  4.8675346e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
           3.8501477e+00,  5.4071717e+00,  5.5333896e+00,
           5.4595857e+00,  3.0910425e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  3.5553482e+00,
           5.4071717e+00,  5.5333896e+00,  5.5333896e+00,
           4.0604429e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  4.0253515e+00,
           5.5333896e+00,  5.5333896e+00,  5.2040067e+00,
           1.0986123e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [13], 'to': [1]}

generate models:255

analyse the exceptions in iter:303
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  19., 164., 253., 255., 253., 118.,  59.,  36.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  78., 251., 251., 253., 251., 251., 251., 199.,  45.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             14., 198., 251., 251., 253., 251., 251., 251., 251., 204.,  26.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              5., 117., 251., 251., 243., 212., 239., 251., 251., 251., 218.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             95., 251., 251., 251., 120.,   0., 175., 251., 251., 251., 231.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  97.,
            237., 251., 251., 251.,   0.,   0.,  67., 240., 251., 251., 243.,
            108.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 163.,
            251., 251., 240.,  81.,   0.,   0.,   0.,  68., 251., 251., 251.,
            179.,   9.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  13., 145., 251.,
            251., 226.,  80.,   0.,   0.,   0.,   0.,  39., 251., 251., 251.,
            251., 115.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 144., 251., 251.,
            251., 173.,   0.,   0.,   0.,   0.,   0.,  18., 167., 251., 251.,
            251., 115.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 233., 251., 251.,
            251., 173.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 251., 251.,
            251., 115.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 176., 253., 253., 216.,
            179.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  99., 253., 253.,
            253., 116.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  55., 210., 251., 251.,  96.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 251., 251.,
            214.,  62.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 117., 251., 251., 251.,  96.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  28., 204., 251., 237.,
             53.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  55., 241., 251., 251., 160.,   7.,
              0.,   0.,   0.,   0.,   0.,   0.,  28., 222., 251., 251., 231.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  59., 251., 251., 251., 153.,   0.,
              0.,   0.,   0.,   0.,  23.,  98., 204., 251., 251., 251., 156.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  59., 251., 251., 251., 153.,   0.,
              0.,   0.,  85., 155., 179., 251., 251., 251., 251., 154.,  15.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  59., 251., 251., 251., 236., 214.,
            214., 214., 234., 251., 253., 251., 251., 248., 156.,  15.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  41., 209., 251., 251., 251., 251.,
            251., 251., 251., 251., 253., 251., 196., 146.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  54., 115., 241., 251., 251.,
            251., 251., 251., 251., 253., 187.,  35.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  83., 251., 251.,
            251., 251., 251., 101.,  57.,  31.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [64, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:257

analyse output arrays in iter:306

pre layer res:
7:add
{'name': 'add', 'output': array([[[[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]],

        [[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]],

        [[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]],

        ...,

        [[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]],

        [[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]],

        [[  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   2540.988 ,
            2540.988 ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ..., 165955.34  ,
           36375.598 ,   1693.9951],
         ...,
         [  1693.9951,   2540.988 , 110615.73  , ..., 233070.6   ,
           98839.6   ,   1693.9951],
         [  1693.9951,   2540.988 ,   2540.988 , ...,   7191.531 ,
            2540.988 ,   1693.9951],
         [  1129.3318,   1693.9951,   1693.9951, ...,   1693.9951,
            1693.9951,   1129.3318]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [5, 10], 'to': [11]}
tf node:
{'name': 'sin', 'output': array([[[[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]],

        [[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]],

        [[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]],

        ...,

        [[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]],

        [[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]],

        [[-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ...,  0.5315417 ,
           0.5315417 , -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.65321606,
           0.78560895, -0.6262056 ],
         ...,
         [-0.6262056 ,  0.5315417 ,  0.24665745, ...,  0.85400075,
          -0.92706424, -0.6262056 ],
         [-0.6262056 ,  0.5315417 ,  0.5315417 , ..., -0.41248262,
           0.5315417 , -0.6262056 ],
         [-0.9974967 , -0.6262056 , -0.6262056 , ..., -0.6262056 ,
          -0.6262056 , -0.9974967 ]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [7], 'to': [6]}
ms node:
{'name': 'sin', 'output': array([[[[-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99666965],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.5395825 ,
           0.5395825 , -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.65187234,
           0.84429175, -0.61683404],
         ...,
         [-0.61683404,  0.5395825 , -0.00858279, ...,  0.3303888 ,
          -0.92996484, -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ..., -0.3932664 ,
           0.5395825 , -0.61683404],
         [-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99670935]],

        [[-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99666965],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.5395825 ,
           0.5395825 , -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.65187234,
           0.84429175, -0.61683404],
         ...,
         [-0.61683404,  0.5395825 , -0.00858279, ...,  0.3303888 ,
          -0.92996484, -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ..., -0.3932664 ,
           0.5395825 , -0.61683404],
         [-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99670935]],

        [[-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99666965],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.5395825 ,
           0.5395825 , -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.65187234,
           0.84429175, -0.61683404],
         ...,
         [-0.61683404,  0.5395825 , -0.00858279, ...,  0.3303888 ,
          -0.92996484, -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ..., -0.3932664 ,
           0.5395825 , -0.61683404],
         [-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99670935]],

        ...,

        [[-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99666965],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.5395825 ,
           0.5395825 , -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ...,  0.65187234,
           0.84429175, -0.61683404],
         ...,
         [-0.61683404,  0.5395825 , -0.00858279, ...,  0.3303888 ,
          -0.92996484, -0.61683404],
         [-0.61683404,  0.5395825 ,  0.5395825 , ..., -0.3932664 ,
           0.5395825 , -0.61683404],
         [-0.99666965, -0.6169301 , -0.6169301 , ..., -0.6169301 ,
          -0.6169301 , -0.99670935]],

        [[-0.9967291 , -0.61548805, -0.61548805, ..., -0.61548805,
          -0.61548805, -0.9966994 ],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ...,  0.5447112 ,
           0.5447112 , -0.61558425],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ...,  0.4299396 ,
           0.80917233, -0.61558425],
         ...,
         [-0.6153918 ,  0.5447112 ,  0.08506418, ...,  0.10216193,
          -0.94617057, -0.61558425],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ..., -0.401332  ,
           0.5447112 , -0.61558425],
         [-0.9966994 , -0.6157766 , -0.6157766 , ..., -0.6157766 ,
          -0.6157766 , -0.99670935]],

        [[-0.9967291 , -0.61548805, -0.61548805, ..., -0.61548805,
          -0.61548805, -0.9966994 ],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ...,  0.5447112 ,
           0.5447112 , -0.61558425],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ...,  0.4299396 ,
           0.80917233, -0.61558425],
         ...,
         [-0.6153918 ,  0.5447112 ,  0.08506418, ...,  0.10216193,
          -0.94617057, -0.61558425],
         [-0.6153918 ,  0.5447112 ,  0.5447112 , ..., -0.401332  ,
           0.5447112 , -0.61558425],
         [-0.9966994 , -0.6157766 , -0.6157766 , ..., -0.6157766 ,
          -0.6157766 , -0.99670935]]]], dtype=float32), 'output_shape': (1, 128, 14, 14), 'from': [7], 'to': [6]}
torch node:
{'name': 'sin', 'output': array([[[[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]],

        [[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]],

        [[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]],

        ...,

        [[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]],

        [[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]],

        [[-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ],
         [-0.59948975,  0.5146885 ,  0.5146885 , ...,  0.5146885 ,
           0.5146885 , -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.33049133,
           0.7698065 , -0.59948975],
         ...,
         [-0.59948975,  0.5146885 , -0.98288506, ...,  0.86104286,
           0.73134017, -0.59948975],
         [-0.59948975,  0.5146885 ,  0.5146885 , ..., -0.19826917,
           0.5146885 , -0.59948975],
         [-0.9978059 , -0.59948975, -0.59948975, ..., -0.59948975,
          -0.59948975, -0.9978059 ]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 14, 14]), 'from': [7], 'to': [6]}

generate models:260

analyse the exceptions in iter:307
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 256, 28, 28] to have 512 channels, but got 256 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:261

analyse the exceptions in iter:317
torch exception:
{'id': 5, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<SinBackward0>)]}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 100] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 5, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:270

analyse the exceptions in iter:319
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],

         [[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],

         [[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],

         ...,

         [[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],

         [[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],

         [[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          ...,
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
          [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]]]],
       grad_fn=<ReluBackward0>)]}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 64, 28, 100] to have 512 channels, but got 64 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 64, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]],
  [[9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]],
  [[9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]],
  ...
  [[9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]],
  [[9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]],
  [[9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   ...
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003],
   [9.99999978e-003, 9.99999978e-003, 9.99999978e-003 ... 9.99999978e-003, 9.99999978e-003, 9.99999978e-003]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 64, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:272

analyse output arrays in iter:322

pre layer res:
4:softmax
{'name': 'softmax', 'output': array([[[[3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 2.6786372e-33, 1.0000000e+00,
          3.6251410e-34, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.1527202e-17,
          9.9817967e-01, 9.1022201e-04, 1.4225698e-21, 8.9694680e-37,
          1.0511448e-20, 9.1022201e-04, 8.5175152e-17, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 2.3802664e-26, 1.0000000e+00, 7.0954738e-23,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 3.5326284e-24, 1.8048515e-35,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          2.6102790e-23, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.5436657e-13,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          5.0000000e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 4.3782554e-27, 5.0000000e-01,
          8.0190541e-29, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.9751079e-01,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 1.6660128e-05, 2.4725823e-03,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.0000000e-01,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 1.3947341e-10, 5.0000000e-01, 9.0242574e-36,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 3.3333334e-01,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          3.4206266e-11, 3.3333334e-01, 3.3333334e-01, 6.0161717e-36,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 3.3333334e-01,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 3.3333334e-01,
          1.4912597e-38, 5.3460361e-29, 3.3333334e-01, 6.0161717e-36,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 7.0201213e-24,
          9.2582471e-15, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 2.0926687e-20, 2.6894143e-01, 0.0000000e+00,
          0.0000000e+00, 2.4166605e-37, 7.3105854e-01, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          4.2231882e-01, 5.8651404e-12, 2.2141692e-22, 1.1780449e-10,
          1.5536243e-01, 1.5943104e-11, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 1.3960595e-37, 4.2231882e-01, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 8.7565109e-27, 2.3195230e-16, 9.8541544e-34,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 3.3057005e-37, 1.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 3.3057005e-37, 1.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 1.8795287e-12, 1.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 1.0000000e+00, 2.7894681e-10, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 1.0000000e+00, 4.7808930e-25, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          4.4737793e-38, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          1.1548224e-17, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          1.1548224e-17, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          1.1548224e-17, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02],
         [3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02,
          3.5714287e-02, 3.5714287e-02, 3.5714287e-02, 3.5714287e-02]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [0], 'to': [3, 5, 6]}
tf node:
{'name': 'log', 'output': array([[[[-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -7.5000000e+01,
           0.0000000e+00, -7.7000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -3.9001823e+01,
          -1.8219847e-03, -7.0018220e+00, -4.8001823e+01,
          -8.3001823e+01, -4.6001823e+01, -7.0018220e+00,
          -3.7001823e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.9000000e+01,  0.0000000e+00, -5.1000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.4000000e+01, -8.0000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.2000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.9000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.9314718e-01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -6.0693146e+01, -6.9314718e-01, -6.4693146e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4923123e-03,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.1002492e+01, -6.0024924e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -6.9314718e-01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.2693148e+01,
          -6.9314718e-01, -8.0693146e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986123e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4098612e+01, -1.0986123e+00,
          -1.0986123e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986123e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.0986123e+00, -8.7098610e+01, -6.5098610e+01,
          -1.0986123e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -5.3313263e+01, -3.2313263e+01,
                    -inf,           -inf,           -inf,
                    -inf, -4.5313263e+01, -1.3132616e+00,
                    -inf,           -inf, -8.4313263e+01,
          -3.1326175e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.6199474e-01,
          -2.5861996e+01, -4.9861996e+01, -2.2861996e+01,
          -1.8619946e+00, -2.4861996e+01,           -inf,
                    -inf,           -inf, -8.4861992e+01,
          -8.6199474e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -6.0000000e+01, -3.6000000e+01, -7.6000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.7000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  0.0000000e+00,
          -2.2000000e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  0.0000000e+00,
          -5.6000000e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -8.6000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00],
         [-3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00, -3.3322046e+00, -3.3322046e+00,
          -3.3322046e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [4], 'to': [8]}
ms node:
{'name': 'log', 'output': array([[[[-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -7.5000000e+01,
          -1.4305115e-06, -7.7000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -3.9001823e+01,
          -1.8235346e-03, -7.0018220e+00, -4.8001823e+01,
          -8.3001823e+01, -4.6001823e+01, -7.0018220e+00,
          -3.7001823e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.9000000e+01, -1.4305115e-06, -5.1000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.4000004e+01, -8.0000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.2000000e+01,
          -1.4305115e-06,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.8999998e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.9314861e-01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -6.0693146e+01, -6.9314861e-01, -6.4693153e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4937429e-03,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.1002496e+01, -6.0024896e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -6.9314861e-01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.2693150e+01,
          -6.9314861e-01, -8.0693146e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986142e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4098614e+01, -1.0986142e+00,
          -1.0986142e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986142e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.0986142e+00, -8.7098610e+01, -6.5098610e+01,
          -1.0986142e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -5.3313259e+01, -3.2313259e+01,
                    -inf,           -inf,           -inf,
                    -inf, -4.5313267e+01, -1.3132646e+00,
                    -inf,           -inf, -8.4313255e+01,
          -3.1326181e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.6199802e-01,
          -2.5861996e+01, -4.9861988e+01, -2.2861998e+01,
          -1.8619939e+00, -2.4861994e+01,           -inf,
                    -inf,           -inf, -8.4861992e+01,
          -8.6199802e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -6.0000000e+01, -3.5999996e+01, -7.6000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
          -1.4305115e-06,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
          -1.4305115e-06,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.6999996e+01,
          -1.4305115e-06,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.4305115e-06,
          -2.2000002e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -1.4305115e-06,
          -5.6000004e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -8.6000000e+01, -1.4305115e-06,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000004e+01, -1.4305115e-06,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000004e+01, -1.4305115e-06,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000004e+01, -1.4305115e-06,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00],
         [-3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00, -3.3322077e+00, -3.3322077e+00,
          -3.3322077e+00]]]], dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [4], 'to': [8]}
torch node:
{'name': 'log', 'output': array([[[[-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -7.5000000e+01,
           0.0000000e+00, -7.7000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -3.9001823e+01,
          -1.8219847e-03, -7.0018220e+00, -4.8001823e+01,
          -8.3001823e+01, -4.6001823e+01, -7.0018220e+00,
          -3.7001823e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.9000000e+01,  0.0000000e+00, -5.1000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -5.4000000e+01, -8.0000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -5.2000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.9000000e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -6.9314718e-01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -6.0693146e+01, -6.9314718e-01, -6.4693146e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4923123e-03,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.1002492e+01, -6.0024924e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -6.9314718e-01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.2693148e+01,
          -6.9314718e-01, -8.0693146e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986123e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -2.4098612e+01, -1.0986123e+00,
          -1.0986123e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -1.0986123e+00,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
          -1.0986123e+00, -8.7098610e+01, -6.5098610e+01,
          -1.0986123e+00, -8.1098610e+01,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -5.3313263e+01, -3.2313263e+01,
                    -inf,           -inf,           -inf,
                    -inf, -4.5313263e+01, -1.3132616e+00,
                    -inf,           -inf, -8.4313263e+01,
          -3.1326166e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.6199474e-01,
          -2.5861996e+01, -4.9861996e+01, -2.2861996e+01,
          -1.8619947e+00, -2.4861996e+01,           -inf,
                    -inf,           -inf, -8.4861992e+01,
          -8.6199474e-01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -9.1000000e+01,
          -6.0000000e+01, -3.6000000e+01, -7.6000000e+01,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -8.4000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf, -2.7000000e+01,
           0.0000000e+00,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  0.0000000e+00,
          -2.2000000e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,  0.0000000e+00,
          -5.6000000e+01,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -8.6000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [          -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf, -3.9000000e+01,  0.0000000e+00,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf,           -inf,           -inf,
                    -inf],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00],
         [-3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00, -3.3322043e+00, -3.3322043e+00,
          -3.3322043e+00]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [4], 'to': [8]}

generate models:274

analyse the exceptions in iter:324
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  55.,  97., 178., 254., 254., 255., 184.,  12.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   3.,  11.,  79., 110., 143.,
            209., 209., 253., 253., 253., 249., 243., 252., 253.,  20.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   3., 152., 253., 253., 253., 253.,
            253., 253., 215., 188., 165.,  49.,  18., 232., 253.,  20.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  89., 253., 253., 240., 143., 133.,
             40.,  34.,  14.,   0.,   0.,   8., 196., 253., 184.,   3.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   4., 143., 131.,  25.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  45., 253., 253., 106.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 127., 253., 229.,  37.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  21., 228., 253.,  96.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  60., 253., 253.,  29.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  45., 134.,  84.,  32.,
             95.,  95.,  95.,  95., 171., 253., 253., 114.,  95.,  95.,  95.,
             41.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 198., 253., 253., 251.,
            253., 253., 253., 253., 253., 253., 253., 253., 253., 253., 253.,
            109.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  71., 148., 148., 148.,
            204., 248., 248., 251., 253., 253., 250., 248., 209., 195., 148.,
             25.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 105., 253., 244.,  32.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 150., 253., 233.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  34., 237., 253.,  73.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 100., 253., 224.,  20.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 149., 253., 193.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  14., 243., 253.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  95., 253., 248.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 191., 253., 191.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  79., 252., 148.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:276

analyse the exceptions in iter:327
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6928e+04, 1.2903e+05,
           1.2903e+05, 1.2903e+05, 1.3005e+05, 1.2903e+05, 1.2903e+05,
           5.6448e+04, 5.8320e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 4.0500e+03, 1.2103e+05, 1.2802e+05,
           1.2802e+05, 1.2802e+05, 1.2802e+05, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 5.6448e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 2.8880e+03, 8.0802e+04, 1.2802e+05, 1.2802e+05,
           9.3312e+04, 2.9768e+04, 3.2000e+03, 8.6528e+04, 1.2802e+05,
           1.2802e+05, 3.7538e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 1.0082e+04, 6.6978e+04, 7.4420e+03,
           1.9220e+03, 0.0000e+00, 6.4800e+02, 8.9042e+04, 1.2802e+05,
           1.2400e+05, 1.6562e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 4.5000e+02, 2.2472e+04, 1.2802e+05, 1.2301e+05,
           8.0802e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00,
           3.9200e+02, 6.2720e+03, 1.1045e+05, 1.2802e+05, 1.0951e+05,
           5.4080e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e+01,
           6.9192e+04, 1.2802e+05, 1.2802e+05, 1.2802e+05, 8.9888e+04,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 1.3122e+04, 8.0000e+04, 1.2301e+05,
           1.2500e+05, 1.2802e+05, 9.4178e+04, 2.4500e+03, 5.0000e+01,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 7.6880e+03, 1.2103e+05, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 1.2802e+05, 1.8818e+04, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 5.2020e+03, 1.1045e+05, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 1.2802e+05, 7.4498e+04, 4.5000e+02, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 5.0000e+03, 9.2480e+03, 9.2480e+03,
           3.5912e+04, 1.0125e+05, 1.2802e+05, 4.7432e+04, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 2.1632e+04, 1.2802e+05, 7.2962e+04, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           3.2000e+01, 2.3328e+04, 1.2802e+05, 1.0215e+05, 3.6980e+03,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e+01,
           6.9938e+04, 1.2802e+05, 1.2802e+05, 4.8050e+04, 3.0420e+03,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e+03, 2.1632e+04,
           1.2500e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8800e+02, 6.6248e+04,
           1.2802e+05, 1.2802e+05, 9.5922e+04, 2.0480e+03, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 3.0420e+03, 9.8568e+04, 1.2802e+05,
           7.4420e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 7.4420e+03, 6.1952e+04, 1.2802e+05,
           1.2802e+05, 9.5922e+04, 3.3620e+03, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 2.7848e+04, 1.2802e+05, 1.2802e+05,
           2.3328e+04, 1.6820e+03, 1.2500e+03, 0.0000e+00, 0.0000e+00,
           9.6800e+02, 2.0000e+04, 1.1424e+05, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 3.5280e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 7.3728e+04, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 9.0738e+04, 8.7362e+04, 6.4082e+04, 6.4082e+04,
           8.4050e+04, 1.2802e+05, 1.2802e+05, 1.2400e+05, 8.6528e+04,
           1.1250e+04, 3.2000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 3.8720e+03, 9.8568e+04, 1.0765e+05,
           1.2301e+05, 1.2802e+05, 1.2802e+05, 1.2802e+05, 1.2802e+05,
           1.2802e+05, 1.2802e+05, 8.9888e+04, 1.4450e+04, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           1.5842e+04, 2.6912e+04, 2.6912e+04, 2.6912e+04, 2.6912e+04,
           2.6912e+04, 2.6912e+04, 1.9220e+03, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00],
          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           0.0000e+00, 0.0000e+00, 0.0000e+00]]]])]}
Given groups=1, weight of size [64, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:279

analyse the exceptions in iter:329
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  84., 255.,
            248.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 150., 253.,
            186.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 150., 241.,
              6.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79., 246., 240.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  30., 244., 253., 240.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  33., 253., 253., 127.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29.,
            192., 154.,   0.,   0.,   0.,   0.,   0.,  77., 253., 247.,  74.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 126.,
            253., 181.,   0.,   0.,   0.,   0.,  13., 191., 253., 193.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28., 120., 246.,
            231.,  63.,   0.,   0.,   0.,   0., 143., 253., 253., 104.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  27., 205., 253., 231.,
             63.,   0.,   0.,   0.,   0.,  46., 191., 253., 199.,  24.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  26., 139., 253., 253., 138.,
              0.,   0.,   0.,   0.,   0., 176., 253., 246.,  75.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  81., 244., 253., 253.,  83.,
             37.,   0.,   0.,   0.,  11., 192., 253., 237.,  50.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  45., 244., 253., 253., 253., 253.,
            229., 176., 176., 176., 192., 253., 253., 214.,  50.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 181., 253., 253., 253., 233., 253.,
            253., 253., 253., 253., 253., 253., 247., 143.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  89., 147., 221., 104.,  44.,  91.,
             91., 128., 221., 245., 253., 253., 197.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  21., 196., 253., 235.,  76.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 196., 253., 253., 154.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  41., 228., 253., 253., 133.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  72., 253., 253., 253.,  78.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  72., 253., 253., 226.,  38.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:281

analyse output arrays in iter:334

pre layer res:
3:empty_seq_operator
{'name': 'empty_seq_operator', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [11], 'to': [10]}
tf node:
{'name': 'softmax', 'output': array([[[[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        ...,

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [3], 'to': [4]}
ms node:
{'name': 'softmax', 'output': array([[[[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        ...,

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [3], 'to': [4]}
torch node:
{'name': 'softmax', 'output': array([[[[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        ...,

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]],

        [[0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.07142857, 0.07142857, 0.07142857, ..., 0.07142857,
          0.07142857, 0.07142857]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [3], 'to': [4]}

generate models:285

analyse output arrays in iter:336

pre layer res:
10:pad
{'name': 'pad', 'output': array([[11777923., 17666888., 17666888., ...,        0.,        0.,
               0.]], dtype=float32), 'output_shape': TensorShape([1, 50176]), 'from': [9], 'to': [14]}
tf node:
{'name': 'cos', 'output': array([[-0.5100917, -0.9424647, -0.9424647, ...,  1.       ,  1.       ,
         1.       ]], dtype=float32), 'output_shape': TensorShape([1, 50176]), 'from': [10], 'to': [11]}
ms node:
{'name': 'cos', 'output': array([[ 0.91117114, -0.95888203, -0.95888203, ...,  1.        ,
         1.        ,  1.        ]], dtype=float32), 'output_shape': (1, 50176), 'from': [10], 'to': [11]}
torch node:
{'name': 'cos', 'output': array([[ 0.0310742 , -0.35477105, -0.35477105, ...,  1.        ,
         1.        ,  1.        ]], dtype=float32), 'output_shape': torch.Size([1, 50176]), 'from': [10], 'to': [11]}

generate models:287

analyse the exceptions in iter:338
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])]}
Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 1, 14, 14] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 14, 14], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:289

analyse the exceptions in iter:339
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsafeViewBackward0>)]}
Given groups=1, weight of size [512, 64, 1, 1], expected input[1, 1, 28, 100] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:290

analyse output arrays in iter:342

pre layer res:
1:conv2d
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [3], 'to': [9]}
tf node:
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [1], 'to': [5]}
ms node:
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': (1, 512, 28, 28), 'from': [1], 'to': [5]}
torch node:
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 28]), 'from': [1], 'to': [5]}

generate models:292

analyse the exceptions in iter:348
torch exception:
{'id': 4, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ReluBackward0>)]}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 512, 28, 28] to have 256 channels, but got 512 channels instead
mindspore exception:
{'id': 4, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:297

analyse the exceptions in iter:349
torch exception:
{'id': 1, 'name': 'linear', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 143., 255., 223., 102.,
           233., 130., 130., 130.,  37.,   7.,   7.,   7.,   7., 116., 130.,
           130.,   6.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   1., 148., 253., 224., 158.,
           243., 253., 253., 253., 253., 253., 253., 253., 253., 253., 253.,
           253.,  18.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,  13., 253., 253., 251.,  86.,
            19., 111., 111., 202., 235., 235., 235., 235., 241., 253., 253.,
           253.,  66.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   4., 142., 253., 253., 149.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  43., 184., 253., 253.,
           127.,   5.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 136., 253., 238.,  39.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  54., 239., 253., 240.,
            64.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 117., 216., 195.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,  31., 172., 253., 239., 127.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0., 123., 253., 253., 132.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,  40., 175., 253., 235.,  24.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0., 199., 253., 253.,  73.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,  81., 239., 253., 189.,  10.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,  48., 239., 253., 192.,  41.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,  17., 119., 253., 231., 161.,  21.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,  16., 179., 253., 253.,  79.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 153., 253., 253., 196.,  10.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            84., 242., 253., 200.,  27.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  82.,
           243., 253., 253., 142.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  13., 200.,
           253., 253., 151.,   7.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 129., 253.,
           253., 231.,  42.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  25., 253., 253.,
           233.,  49.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  86., 253., 253.,
            89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
mat1 and mat2 shapes cannot be multiplied (28x28 and 100x100)
mindspore exception:
{'id': 1, 'name': 'linear', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 100. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 100](transpose_b=True).

generate models:298

analyse output arrays in iter:353

pre layer res:
8:log
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [2], 'to': [6]}
tf node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [8], 'to': [3]}
ms node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': (1, 512, 28, 28), 'from': [8], 'to': [3]}
torch node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 28]), 'from': [8], 'to': [3]}

generate models:301

analyse the exceptions in iter:354
torch exception:
{'id': 0, 'name': 'linear', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             82., 254., 131.,  21.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  41.,
            243., 253., 130.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 214.,
            253., 142.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 102., 253.,
            252.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 204., 255.,
            253.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  41., 243., 253.,
            171.,   0.,   0.,   0.,   0.,  21., 142., 123.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  52., 253., 254.,
            112.,   0.,   0.,   0.,  21., 173., 253., 142.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 232., 253.,
            151.,   0.,   0.,   0., 102., 253., 252., 102.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  82., 254.,
            253.,  51.,  51.,  51., 213., 254., 253., 224.,  20.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 131.,
            252., 253., 252., 253., 252., 253., 252.,  81.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             41.,  82., 203., 234., 253., 254.,  91.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 152., 252., 253.,  50.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 152., 253., 254.,  50.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 152., 252., 233.,  30.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  92., 253., 203.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  51., 252., 203.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 204., 204.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 203., 243.,  40.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  82., 234., 112.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  51., 212.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
mat1 and mat2 shapes cannot be multiplied (28x28 and 50x100)
mindspore exception:
{'id': 0, 'name': 'linear', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'MatMul', the input dimensions must be equal, but got 'x1_col': 28 and 'x2_row': 50. And 'x' shape [28, 28](transpose_a=False), 'y' shape [100, 50](transpose_b=True).

generate models:302

analyse output arrays in iter:356

pre layer res:
10:pad
{'name': 'pad', 'output': array([[512.,  inf,  inf, ...,   0.,   0.,   0.]], dtype=float32), 'output_shape': TensorShape([1, 25088]), 'from': [13], 'to': [12]}
tf node:
{'name': 'softmax', 'output': array([[nan, nan, nan, ..., nan, nan, nan]], dtype=float32), 'output_shape': TensorShape([1, 25088]), 'from': [10], 'to': [17]}
ms node:
{'name': 'softmax', 'output': array([[ 0., nan, nan, ...,  0.,  0.,  0.]], dtype=float32), 'output_shape': (1, 25088), 'from': [10], 'to': [17]}
torch node:
{'name': 'softmax', 'output': array([[nan, nan, nan, ..., nan, nan, nan]], dtype=float32), 'output_shape': torch.Size([1, 25088]), 'from': [10], 'to': [17]}

generate models:304

analyse the exceptions in iter:357
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 218., 253., 124.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  84., 236., 251., 251.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  63., 236., 251., 251., 122.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  73., 251., 251., 251., 173.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 202., 251., 251., 251.,  71.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  53., 255., 253., 253., 253.,  72.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 180., 253., 251., 251., 188.,  30.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 180., 253., 251., 251., 142.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  47., 211., 253., 251., 235.,  82.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  27., 211., 251., 253., 251., 215.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  89., 253., 253., 255., 253., 164.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 217., 251., 251., 253., 168.,  15.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             21., 221., 251., 251., 253., 107.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  32.,
            190., 251., 251., 251., 221.,  61.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73.,
            251., 251., 251., 251., 180.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.,
            253., 253., 253., 201.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 105., 253.,
            251., 251., 251.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 180., 253.,
            251., 246., 137.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 180., 253.,
            251., 215.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 180., 253.,
            251.,  86.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [64, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:305

analyse the exceptions in iter:366
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [512, 128, 1, 1], expected input[1, 64, 28, 28] to have 128 channels, but got 64 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 64, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 64, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:313

analyse the exceptions in iter:379
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 230., 145.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  62., 249., 106.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 218., 252.,  71.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  32., 254., 234.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 128., 254.,  85.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  26., 212., 232.,  18.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             51.,  12.,   0.,   0.,   0.,   3., 141., 254., 170.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  74.,
            208., 166.,   0.,   0.,   0., 111., 254., 234.,  25.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14., 210.,
            254.,  88.,   0.,   0.,   3., 191., 254., 178.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 194., 254.,
            181.,  34.,   0.,   0., 121., 254., 236.,  52.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  33., 242., 255.,
            181., 100.,  16.,   0., 203., 254., 166.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 224., 254., 254.,
            254., 254., 249., 167., 254., 248.,  54.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  37., 254., 245., 166.,
            124., 188., 254., 254., 254., 184.,  89.,  89.,  26.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 120.,  65.,   0.,
              0.,   4., 130., 254., 254., 224., 223., 155.,   5.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  45., 243., 254., 109.,   2.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 156., 254., 207.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              5., 190., 253., 106.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            106., 254., 245.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            186., 254., 122.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            181., 203.,   8.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:325

analyse output arrays in iter:382

pre layer res:
7:pad
{'name': 'pad', 'output': array([[1.        , 0.73105854, 0.73105854, 0.73105854, 0.73105854,
        0.73105854, 0.73105854, 0.73105854, 0.73105854, 0.73105854,
        0.73105854, 0.73105854, 0.73105854, 0.73105854, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        ]], dtype=float32), 'output_shape': TensorShape([1, 196]), 'from': [11], 'to': [15]}
tf node:
{'name': 'log', 'output': array([[ 0.        , -0.31326175, -0.31326175, -0.31326175, -0.31326175,
        -0.31326175, -0.31326175, -0.31326175, -0.31326175, -0.31326175,
        -0.31326175, -0.31326175, -0.31326175, -0.31326175,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf]], dtype=float32), 'output_shape': TensorShape([1, 196]), 'from': [7], 'to': [8]}
ms node:
{'name': 'log', 'output': array([[-1.4305115e-06, -3.1326172e-01, -3.1326172e-01, -3.1326172e-01,
        -3.1326172e-01, -3.1326172e-01, -3.1326172e-01, -3.1326172e-01,
        -3.1326172e-01, -3.1326172e-01, -3.1326172e-01, -3.1326172e-01,
        -3.1326172e-01, -3.1326172e-01,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf,
                  -inf,           -inf,           -inf,           -inf]],
      dtype=float32), 'output_shape': (1, 196), 'from': [7], 'to': [8]}
torch node:
{'name': 'log', 'output': array([[ 0.        , -0.31326166, -0.31326166, -0.31326166, -0.31326166,
        -0.31326166, -0.31326166, -0.31326166, -0.31326166, -0.31326166,
        -0.31326166, -0.31326166, -0.31326166, -0.31326166,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf,        -inf,        -inf,        -inf,        -inf,
               -inf]], dtype=float32), 'output_shape': torch.Size([1, 196]), 'from': [7], 'to': [8]}

generate models:327

analyse the exceptions in iter:387
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 118., 253.,
            253., 253., 253., 255., 127., 121., 121., 121.,  95.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 121., 249., 252.,
            252., 195., 252., 253., 252., 252., 252., 252., 249., 131.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200., 252., 252.,
            109.,  12.,  26., 159., 158., 193., 252., 252., 252., 212.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 216., 252., 176.,
             15.,   0.,   0.,   0.,   0.,  15., 107., 252., 252., 212.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 150., 252., 209.,  17.,
              0.,   0.,   0.,   0.,   0.,   0., 116., 252., 252., 176.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 171.,  87.,   0.,
              0.,   0.,   0.,   0.,   0.,  20., 221., 252., 241.,  63.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 185., 252., 252., 108.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  12., 229., 252., 252.,  66.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 184., 252., 252., 213.,  28.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  64., 246., 252., 252., 115.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 184., 253., 253., 173.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   6., 107., 253., 252., 212.,  29.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  63., 252., 253., 212.,  30.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             65., 236., 252., 215.,  29.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,
            184., 252., 252.,  99.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  40., 197.,
            252., 249., 128.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75., 252.,
            252., 217.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 228., 252.,
            221.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200., 252., 221.,
             39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200., 224.,  38.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:332

analyse the exceptions in iter:407
torch exception:
{'id': 2, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [64, 64, 1, 1], expected input[1, 256, 28, 100] to have 64 channels, but got 256 channels instead
mindspore exception:
{'id': 2, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 100], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:348

analyse the exceptions in iter:424
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,
             67., 228., 254., 255., 226.,  39.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75., 132., 172.,
            253., 253., 253., 253., 253., 203.,  44.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 155., 198., 228., 253., 253.,
            253., 246., 253., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  40., 214., 253., 253., 253., 227.,
            177.,  46., 205., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  38., 199., 253., 253., 253., 136.,  75.,
              0.,   0.,  95., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  67., 253., 253., 253., 253., 201.,  19.,
             19.,  19., 107., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  67., 253., 253., 253., 253., 253., 253.,
            253., 253., 253., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  27., 180., 253., 253., 253., 253., 253.,
            253., 253., 253., 253., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  19.,  43., 224., 224., 224., 224.,
            224., 203., 107., 242., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 160., 253., 253.,  84.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 160., 253., 253., 190.,  11.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  17., 201., 253., 253., 228., 126.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  38., 253., 253., 253., 186.,  31.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  38., 253., 253., 253., 253.,  74.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  31., 225., 253., 253., 253., 108.,   2.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  58., 215., 253., 253., 253.,  93.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0., 170., 253., 253., 253., 196.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   6., 236., 253., 253., 251.,
            127.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 236., 253., 253., 253.,
            130.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  60., 148., 253.,  70.,
             34.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:362

analyse the exceptions in iter:435
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  60.,  96.,  96.,  48.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  16., 171., 228., 253., 251., 220.,  51.,  32.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 127., 251., 251., 253., 251., 251., 251., 251.,  80.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24.,
            182., 236., 251., 211., 189., 236., 251., 251., 251., 242., 193.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 100., 194.,
            251., 251., 211.,  35.,   0.,  71., 173., 251., 251., 253., 240.,
            158.,  19.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  64., 253., 255.,
            253., 205.,  19.,   0.,   0.,   0.,   0.,  40., 218., 255., 253.,
            253.,  91.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  16., 186., 251., 253.,
            247., 110.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 233., 251.,
            251., 188.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  16., 189., 251., 251., 205.,
            110.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 220.,
            251., 220.,  48.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  72., 251., 251., 251., 158.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,
            251., 251., 232.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 190., 251., 251., 251.,  59.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  32.,
            251., 251., 251.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  96., 253., 253., 253.,  95.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  32.,
            253., 253., 193.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 214., 251., 251., 204.,  23.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  52.,
            251., 251.,  94.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 253., 251., 251., 109.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 221.,
            251., 219.,  47.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 253., 251., 251.,  70.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 234., 251.,
            251., 188.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 253., 251., 251., 188.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  40., 158., 253., 251.,
            172.,  70.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 191., 253., 253., 253.,  96.,
             24.,   0.,   0.,   0.,   0.,  12., 174., 253., 253., 255., 221.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  71., 251., 251., 251., 253.,
            205., 190., 190., 190., 191., 197., 251., 251., 231., 221.,  93.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  16., 126., 236., 251., 253.,
            251., 251., 251., 251., 253., 251., 251., 140.,  47.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  67., 188., 189.,
            188., 188., 188., 188., 189., 188., 109.,   4.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 128, 1, 1], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:372

analyse output arrays in iter:436

pre layer res:
16:reshape
{'name': 'reshape', 'output': array([[[[2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         ...,
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.]],

        [[2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         ...,
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.]],

        [[2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         ...,
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.],
         [2., 2., 2., ..., 2., 2., 2.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [15], 'to': [13]}
tf node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [16], 'to': [11]}
ms node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': (1, 512, 28, 28), 'from': [16], 'to': [11]}
torch node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 28]), 'from': [16], 'to': [11]}

generate models:373

analyse the exceptions in iter:458
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  69., 150., 180.,
            255., 254., 255., 254., 254., 237., 150., 138.,  33.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  29., 239., 253., 253.,
            253., 253., 253., 253., 253., 253., 253., 253., 204.,  48.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  17., 202., 253., 253.,
            253., 253., 238., 217., 217., 217., 245., 253., 253., 240.,  61.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  84., 253., 253.,
            248., 145.,  38.,   0.,   0.,   0.,  51., 209., 253., 253., 236.,
             27.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 143., 253., 253.,
            238.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 177., 253., 253.,
            100.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 109., 253., 253., 253.,
            135.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79., 253., 253.,
            250.,  65.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   6., 190., 253., 253., 253.,
             31.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10., 225., 253.,
            253.,  93.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  94., 253., 253., 253., 236.,
             23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 125., 253.,
            253.,  93.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 138., 253., 253., 253.,  93.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 125., 253.,
            253.,  93.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 227.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 188., 253.,
            251.,  74.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 227.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 228., 253.,
            247.,  37.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 227.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  77., 253., 253.,
            253.,  93.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 227.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 115., 253., 253.,
            246.,  23.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 241.,  54.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  30., 220., 253., 253.,
            230.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 197., 253., 253., 253., 181.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 167., 253., 253., 253.,
             65.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  97., 253., 253., 253., 246.,
             36.,   0.,   0.,   0.,   0.,   4., 135., 252., 253., 252., 135.,
              4.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 174., 253., 253., 253.,
            215.,  60.,   0.,  14.,  68., 183., 253., 253., 253., 205.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  78., 219., 253., 253.,
            253., 230., 218., 225., 253., 253., 253., 248., 211.,  29.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  65., 226., 253.,
            253., 253., 253., 253., 253., 253., 174.,  86.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24.,  46.,
            137., 218., 209., 149., 149.,  91.,  12.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [64, 64, 1, 1], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:393

analyse the exceptions in iter:459
tensorflow exception:
{'id': 3, 'name': 'conv2d', 'framework': 'tensorflow', 'input_datas': <tf.Tensor: shape=(1, 1, 1, 28), dtype=float32, numpy=
array([[[[   0.,    0.,    0.,    0.,    0.,    0., 1660., 2264.,
          2034., 1427., 1259.,  857., 1080., 1487., 1976., 2209.,
          1681.,  805.,  693.,  560.,  689.,  749.,  953., 1093.,
          1446., 1249.,    0.,    0.]]]], dtype=float32)>}
Input 0 of layer conv2d_4494 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (1, 1, 28)
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28], dtype=Float32, value=
[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 1.24900000e+003, 0.00000000e+000, 0.00000000e+000]]])]}
For primitive[Conv2D], the x shape size must be equal to 4, but got 3.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\utils\check_convert_utils.cc:385 CheckInteger


generate models:394

analyse the exceptions in iter:467
torch exception:
{'id': 3, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])]}
Given groups=1, weight of size [256, 128, 1, 1], expected input[1, 256, 28, 28] to have 128 channels, but got 256 channels instead
mindspore exception:
{'id': 3, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:401

analyse the exceptions in iter:474
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [512, 256, 1, 1], expected input[1, 512, 28, 28] to have 256 channels, but got 512 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:407

analyse the exceptions in iter:484
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  38., 237., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 186.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 212.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 212.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 212.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 170., 254., 213.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 126., 253., 212.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 117., 253., 115.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 132.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 212.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  65., 255., 213.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  64., 253., 176.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 118., 253., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 169., 253., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 169., 253., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 170., 255., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 169., 253., 107.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 169., 253., 119.,  15.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 169., 253., 254., 168.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  55., 236., 130.,  28.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:416

analyse the exceptions in iter:485
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  22., 114., 148., 255.,
            218.,  30.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  50., 121., 221., 252., 252., 253.,
            252., 126.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   2.,  48., 171., 245., 252., 252., 247., 143., 144.,
            231.,  72.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 119., 252., 252., 253., 231., 180.,  63.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             62., 141., 249., 252., 208.,  86.,  28.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80.,
            210., 253., 241.,  80.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  22.,  43.,   7.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 112., 242.,
            253., 224.,  66.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  50.,
            121., 221., 231.,  28.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 252., 252.,
            153.,  14.,   0.,   0.,   0.,   0.,   0.,   2.,  48., 215., 245.,
            210., 118.,  16.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 127., 252., 226.,
              0.,   0.,   0.,   0.,   0.,   0.,  57., 176., 252., 226., 128.,
             56.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 171., 252., 235.,
              0.,   0.,   0.,   0.,  45., 168., 225., 252., 182.,  42.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  22., 253., 253.,
            237.,  55.,  48., 218., 253., 247., 176.,  53.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6., 154., 252.,
            253., 231., 247., 231., 168.,  53.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 218.,
            253., 252., 252., 101.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  41., 190., 252.,
            253., 252., 252., 231.,  28.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  36., 223., 252., 252.,
            191., 103., 244., 252.,  86.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  25., 227., 253., 225., 124.,
              0.,   0.,  66., 253., 253.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 123., 252., 236.,  21.,   0.,
              0.,   0.,  22., 252., 252.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 148., 252., 189.,  22.,  22.,
            128., 162., 234., 252., 190.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 122., 252., 252., 252., 252.,
            253., 252., 247., 189.,  31.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   7., 156., 252., 252., 164.,
            191., 147.,  47.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:417

analyse the exceptions in iter:497
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  44., 191., 254., 176., 101.,  16.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  78., 252., 253., 252., 252., 212., 176.,  90.,  67.,  55.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  47., 196., 154., 222., 252., 252., 252., 252., 252., 245.,
            175.,  13.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   5.,   0.,   7.,  54., 129., 230., 238., 252., 252.,
            252.,  33.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 113., 252., 252.,
            202.,  12.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  82., 241., 252., 237.,
             66.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  26., 245., 252., 252., 151.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  61., 206., 252., 252., 154.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 131., 213., 252., 252., 153.,   6.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 173., 243., 252., 243., 119.,   7.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  59., 191., 255., 253., 240., 106.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             60., 233., 252., 253., 206.,  66.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 144.,
            236., 252., 252., 139.,  13.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 171., 251.,
            252., 236.,  87.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 222., 252., 250.,
            130.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  74., 222., 252., 252.,  69.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  32., 230., 252., 226.,  93.,   2.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   6., 147., 252., 229., 115.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  34., 252., 252., 121.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,  19., 171., 252., 121.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [512, 512, 1, 1], expected input[1, 1, 28, 28] to have 512 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 512, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:426

analyse the exceptions in iter:499
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [64, 256, 1, 1], expected input[1, 512, 28, 28] to have 256 channels, but got 512 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:428

final statics:
total operators:28
tensorflow --> nums:27,distinct_bugs:6
mindspore --> nums:79,distinct_bugs:6
torch --> nums:78,distinct_bugs:7
tensorflow --> 
cos:4
log:7
sin:3
conv2d:7
softmax:5
linear:1
mindspore --> 
conv2d:49
cos:4
log:9
linear:10
softmax:5
sin:2
torch --> 
conv2d:48
cos:4
log:7
linear:10
softmax:5
flatten:2
sin:2

generate models:428
