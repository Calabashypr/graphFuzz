
analyse output arrays in iter:7

pre layer res:
3:relu
{'name': 'relu', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 1024, 28, 28]), 'from': [16], 'to': [4]}
tf node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          7.3400320e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 3.5284787e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 3.6720640e+06, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [3], 'to': [5]}
ms node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379704e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5691136e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': (1, 128, 14, 14), 'from': [3], 'to': [5]}
torch node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          3.1467520e+06, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.4379413e+08, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 2.5690780e+07, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 14, 14]), 'from': [3], 'to': [5]}

generate models:3

final statics:
total operators:28
tensorflow --> nums:1,distinct_bugs:1
mindspore --> nums:0,distinct_bugs:0
torch --> nums:0,distinct_bugs:0
tensorflow --> 
conv2d:1
mindspore --> 
torch --> 

generate models:3

analyse output arrays in iter:21

pre layer res:
5:relu
{'name': 'relu', 'output': array([[[[1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 2.9809578e+03, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          7.8962965e+13,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
                    inf,           inf,           inf,           inf,
                    inf, 1.0000000e+00,           inf,           inf,
                    inf,           inf, 1.0000000e+00,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf,           inf,           inf,
          3.0250773e+36, 1.0000000e+00,           inf,           inf,
                    inf, 2.9809578e+03, 1.0000000e+00, 2.6489125e+10,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf,           inf, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.8586719e+31, 9.4961178e+19,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.6516362e+38,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 2.6489125e+10,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00,           inf,
                    inf,           inf, 8.4383568e+26, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 5.5406215e+34,           inf,
                    inf,           inf, 9.4961178e+19, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 5.5406215e+34,           inf,           inf,
                    inf,           inf, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 2.6489125e+10,
                    inf,           inf,           inf,           inf,
          7.4984176e+33, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf, 7.4984176e+33,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
                    inf,           inf, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 2.8307530e+23,
                    inf,           inf,           inf,           inf,
                    inf,           inf,           inf,           inf,
          1.0686475e+13, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00,           inf,           inf,           inf,
                    inf,           inf,           inf, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],
         [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,
          1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00]]]],
      dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [7], 'to': [3]}
tf node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [5], 'to': [1]}
ms node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan,        nan, 0.        ,        nan,
                 nan,        nan,        nan, 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan, 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [5], 'to': [1]}
torch node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [5], 'to': [1]}

generate models:7

analyse the exceptions in iter:35
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29., 141.,
            198., 255., 198.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,  86., 141., 198., 255., 255.,
            255., 255., 170.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  29., 141., 226., 255., 255., 255., 255., 198.,
             86.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 170., 255., 255., 170.,  86.,  86.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 141., 226., 170.,  57.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  86., 255., 198.,  29.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 198., 255., 141.,  86.,  57.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            170., 255., 198., 114., 226., 170.,  29.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 198.,
            255., 114.,  29.,   0., 141., 255.,  29.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 114., 255.,
            114.,   0.,   0.,   0., 141., 255.,  29.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29.,
              0.,   0.,   0.,   0., 226., 255.,  29.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 114., 255., 141.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  86.,   0.,   0.,   0.,   0.,   0.,
              0.,   0., 114., 226., 226.,  29.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 255., 198.,  86.,   0.,   0.,   0.,
            141., 255., 255., 170.,  29.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 226., 255., 226., 170., 226., 255.,
            255., 198.,  29.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  86., 198., 255., 255., 170., 141.,
             57.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [2048, 2048, 1, 1], expected input[1, 1, 28, 28] to have 2048 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 2048, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:13

final statics:
total operators:28
tensorflow --> nums:2,distinct_bugs:2
mindspore --> nums:2,distinct_bugs:2
torch --> nums:2,distinct_bugs:2
tensorflow --> 
conv2d:1
softmax:1
mindspore --> 
softmax:1
conv2d:1
torch --> 
softmax:1
conv2d:1

generate models:18

analyse the exceptions in iter:88
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   8.,
             76., 202., 254., 255., 163.,  37.,   2.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  13., 182.,
            253., 253., 253., 253., 253., 253.,  23.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  15., 179., 253.,
            253., 212.,  91., 218., 253., 253., 179., 109.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 105., 253., 253.,
            160.,  35., 156., 253., 253., 253., 253., 250., 113.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 212., 253., 253.,
             88., 121., 253., 233., 128.,  91., 245., 253., 248., 114.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 104., 253., 253., 110.,
              2., 142., 253.,  90.,   0.,   0.,  26., 199., 253., 248.,  63.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   1., 173., 253., 253.,  29.,
              0.,  84., 228.,  39.,   0.,   0.,   0.,  72., 251., 253., 215.,
             29.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  36., 253., 253., 203.,  13.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  82., 253., 253.,
            170.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  36., 253., 253., 164.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 198., 253.,
            184.,   6.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  36., 253., 253.,  82.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 138., 253.,
            253.,  35.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 128., 253., 253.,  47.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 253.,
            253.,  35.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 154., 253., 253.,  47.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 253.,
            253.,  35.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0., 102., 253., 253.,  99.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 253.,
            253.,  35.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  36., 253., 253., 164.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16., 208., 253.,
            211.,  17.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  32., 244., 253., 175.,   4.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  44., 253., 253.,
            156.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171., 253., 253.,  29.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  30., 217., 253., 188.,
             19.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171., 253., 253.,  59.,
              0.,   0.,   0.,   0.,   0.,   0.,  60., 217., 253., 253.,  70.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  78., 253., 253., 231.,
             48.,   0.,   0.,   0.,  26., 128., 249., 253., 244.,  94.,  15.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   8., 151., 253., 253.,
            234., 101., 121., 219., 229., 253., 253., 201.,  80.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38., 232., 253.,
            253., 253., 253., 253., 253., 253., 201.,  66.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 1, 28, 28] to have 64 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:30

analyse output arrays in iter:99

pre layer res:
14:add
{'name': 'add', 'output': array([[[[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]],

        [[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]],

        [[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]],

        ...,

        [[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]],

        [[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]],

        [[149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         ...,
         [149797.47, 224695.34, 224695.34, ..., 224695.34, 224695.34,
          149797.47],
         [224698.  , 337044.34, 337044.34, ..., 337044.34, 337044.34,
          224698.  ],
         [149801.06, 224698.  , 224698.  , ..., 224698.  , 224698.  ,
          149801.06]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [4, 17], 'to': [23]}
tf node:
{'name': 'softmax', 'output': array([[[[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        ...,

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [14], 'to': [5]}
ms node:
{'name': 'softmax', 'output': array([[[[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        ...,

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03792643, 0.03792643, ..., 0.03792643,
          0.03792643, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]]]], dtype=float32), 'output_shape': (1, 64, 28, 28), 'from': [14], 'to': [5]}
torch node:
{'name': 'softmax', 'output': array([[[[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        ...,

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]],

        [[0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ],
         [0.        , 0.03846154, 0.03846154, ..., 0.03846154,
          0.03846154, 0.        ]]]], dtype=float32), 'output_shape': torch.Size([1, 64, 28, 28]), 'from': [14], 'to': [5]}

generate models:34

final statics:
total operators:28
tensorflow --> nums:3,distinct_bugs:2
mindspore --> nums:4,distinct_bugs:2
torch --> nums:4,distinct_bugs:2
tensorflow --> 
conv2d:1
softmax:2
mindspore --> 
softmax:2
conv2d:2
torch --> 
softmax:2
conv2d:2

generate models:34

analyse the exceptions in iter:113
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  28., 247., 255., 165.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  47., 221., 252., 252., 164.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 177., 252., 252., 252., 164.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 177., 252., 252., 223.,  78.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0., 177., 252., 252., 197.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 114., 236., 252., 235.,  42.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   5., 148., 252., 252., 230.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  14., 135., 252., 252., 252., 230.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  78., 252., 252., 252., 252., 162.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  78., 252., 252., 252., 252.,   9.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  78., 252., 252., 252., 252.,   9.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             32., 200., 252., 252., 252., 105.,   3.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,
            218., 252., 252., 252., 105.,   8.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 225.,
            252., 252., 252., 240.,  69.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  44., 237.,
            252., 252., 228.,  85.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  59., 218., 252.,
            252., 225.,  93.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  65., 208., 252., 252.,
            252., 175.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 133., 252., 252., 252.,
            225.,  68.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 133., 252., 252., 244.,
             54.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 133., 252., 252.,  48.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:39

analyse output arrays in iter:119

pre layer res:
3:conv2d
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 1024, 28, 28]), 'from': [2], 'to': [4]}
tf node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          5.3418656e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.9527900e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [3], 'to': [5]}
ms node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [3], 'to': [5]}
torch node:
{'name': 'conv2d', 'output': array([[[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        ...,

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],

        [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          9.3147103e+10, 0.0000000e+00, 0.0000000e+00],
         ...,
         [0.0000000e+00, 0.0000000e+00, 2.7648852e+10, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],
         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [3], 'to': [5]}

generate models:41

analyse the exceptions in iter:132
torch exception:
{'id': 14, 'name': 'flatten', 'frame_work': 'torch', 'input_datas': [tensor([1.6961e+11, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
        0.0000e+00], grad_fn=<ConstantPadNdBackward0>)]}
Dimension out of range (expected to be in range of [-1, 0], but got 1)

generate models:47

analyse the exceptions in iter:138
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,  17.,  79., 150., 255., 224.,  29.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   9.,  36., 135., 244., 253., 253., 253., 244.,  45.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             5.,  73., 191., 253., 253., 253., 253., 247., 200.,  94.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            57., 253., 253., 253., 253., 222., 177.,  57.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 102.,
           233., 253., 253., 226., 121.,  26.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 125.,
           253., 253., 145.,  20.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   9.,
           196., 253., 251., 207.,  81.,   1.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            22., 233., 253., 253., 253., 100.,  12.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,  16., 176., 244., 253., 253., 144.,  18.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 113., 253., 253., 253., 195.,  21.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,  18.,  59., 200., 253., 253., 215.,  54.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,  18., 205., 253., 253., 194.,  20.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 209., 231., 231.,
           102.,   0.,   0.,   0.,   0.,   0.,  18., 206., 253., 253., 105.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 168., 252., 253., 248., 183.,
            55.,   0.,   0.,   0.,   0.,   0.,   0.,  32., 253., 253., 170.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 171., 253., 253., 209.,  21.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,  44., 253., 253., 170.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,  85., 241., 253., 253., 152.,
            21.,   0.,   0.,   0.,   0.,   0.,  30., 217., 253., 241.,  84.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  86., 241., 253., 253.,
           203.,  66.,  20.,  20.,  66.,  66., 217., 253., 253., 103.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 244., 253.,
           253., 253., 217., 217., 253., 253., 253., 244.,  94.,  15.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31.,  95.,
           253., 253., 253., 253., 253., 253., 201.,  80.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,
            28., 135., 218., 217., 135.,  28.,   9.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:51

analyse output arrays in iter:146

pre layer res:
14:reshape
{'name': 'reshape', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]], dtype=float32), 'output_shape': TensorShape([1, 2048, 28, 28]), 'from': [21], 'to': [19]}
tf node:
{'name': 'softmax', 'output': array([[[[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': TensorShape([1, 2048, 28, 28]), 'from': [14], 'to': [7]}
ms node:
{'name': 'softmax', 'output': array([[[[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': (1, 2048, 28, 28), 'from': [14], 'to': [7]}
torch node:
{'name': 'softmax', 'output': array([[[[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        [[       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         ...,
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan],
         [       nan,        nan,        nan, ...,        nan,
                 nan,        nan]],

        ...,

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]],

        [[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         ...,
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, ..., 0.03571429,
          0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': torch.Size([1, 2048, 28, 28]), 'from': [14], 'to': [7]}

generate models:52

analyse the exceptions in iter:148
torch exception:
{'id': 5, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]]], grad_fn=<AddBackward0>)]}
Given groups=1, weight of size [64, 64, 1, 1], expected input[1, 512, 28, 28] to have 64 channels, but got 512 channels instead
mindspore exception:
{'id': 5, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 512, 28, 28], dtype=Float32, value=
[[[[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  ...
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 512, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:53

analyse output arrays in iter:152

pre layer res:
15:reshape
{'name': 'reshape', 'output': array([[inf,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 'output_shape': TensorShape([1, 50176]), 'from': [14], 'to': [16]}
tf node:
{'name': 'softmax', 'output': array([[nan, nan, nan, ..., nan, nan, nan]], dtype=float32), 'output_shape': TensorShape([1, 50176]), 'from': [15], 'to': [6]}
ms node:
{'name': 'softmax', 'output': array([[nan,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 'output_shape': (1, 50176), 'from': [15], 'to': [6]}
torch node:
{'name': 'softmax', 'output': array([[nan, nan, nan, ..., nan, nan, nan]], dtype=float32), 'output_shape': torch.Size([1, 50176]), 'from': [15], 'to': [6]}

generate models:55

analyse output arrays in iter:187

pre layer res:
21:transpose
{'name': 'transpose', 'output': array([[[[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]],

        [[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]],

        [[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]],

        ...,

        [[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]],

        [[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]],

        [[0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         ...,
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935],
         [0.62245935, 0.62245935, 0.62245935, ..., 0.62245935,
          0.62245935, 0.62245935]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [2], 'to': [3]}
tf node:
{'name': 'conv2d', 'output': array([[[[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]],

        [[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]],

        [[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]],

        ...,

        [[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]],

        [[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]],

        [[1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         ...,
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [1434.375, 1434.375, 1434.375, ..., 1434.375, 1434.375,
           956.25 ],
         [ 956.25 ,  956.25 ,  956.25 , ...,  956.25 ,  956.25 ,
           637.5  ]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [21], 'to': [4]}
ms node:
{'name': 'conv2d', 'output': array([[[[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]],

        [[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]],

        [[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]],

        ...,

        [[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]],

        [[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]],

        [[ 637.3984,  956.0976,  956.0976, ...,  956.0976,  956.0976,
           956.0976],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         ...,
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431],
         [ 956.0976, 1434.1431, 1434.1431, ..., 1434.1431, 1434.1431,
          1434.1431]]]], dtype=float32), 'output_shape': (1, 128, 14, 14), 'from': [21], 'to': [4]}
torch node:
{'name': 'conv2d', 'output': array([[[[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]],

        [[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]],

        [[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]],

        ...,

        [[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]],

        [[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]],

        [[ 637.3954,  956.0829,  956.0829, ...,  956.0829,  956.0829,
           956.0829],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         ...,
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141],
         [ 956.0829, 1434.1141, 1434.1141, ..., 1434.1141, 1434.1141,
          1434.1141]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 14, 14]), 'from': [21], 'to': [4]}

generate models:67

analyse output arrays in iter:188

pre layer res:
7:conv2d
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 1024, 28, 28]), 'from': [9], 'to': [8]}
tf node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 14, 14]), 'from': [7], 'to': []}
ms node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': (1, 128, 14, 14), 'from': [7], 'to': []}
torch node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 128, 14, 14]), 'from': [7], 'to': []}

generate models:68

analyse the exceptions in iter:201
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 210., 253., 163.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0., 198., 252., 252., 162.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,  10.,  86., 242., 252., 252.,  66.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 164., 252., 252., 252., 188.,   8.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  53., 242., 252., 252., 225.,  14.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  78., 252., 252., 252., 204.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  56., 231., 252., 252., 212.,  35.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 157., 252., 252., 252.,  37.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   8., 132., 253., 252., 252., 230.,  24.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  45., 252., 253., 252., 154.,  55.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   7.,  55.,   0.,   0.,   0.,
              0., 107., 253., 255., 228.,  53.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  15.,  24.,  23.,   0.,   0.,   0.,
            110., 242., 252., 228.,  59.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  57.,  83.,   0.,   0.,   0.,  88.,
            247., 252., 252., 140.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  15., 189.,
            252., 252., 252.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  74., 252.,
            252., 238.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 178., 252.,
            252., 189.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  40., 217., 252.,
            252.,  59.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75., 252., 252.,
            252.,  85.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  62., 239., 252.,
            156.,  14.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 178., 252.,
             14.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 3, 3], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:72

analyse the exceptions in iter:236
torch exception:
{'id': 4, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          ...,
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          ...,
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan]],

         [[nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          ...,
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan],
          [nan, nan, nan,  ..., nan, nan, nan]],

         ...,

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]],

         [[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]]], grad_fn=<ReshapeAliasBackward0>)]}
Given groups=1, weight of size [2048, 2048, 1, 1], expected input[1, 256, 28, 28] to have 2048 channels, but got 256 channels instead
mindspore exception:
{'id': 4, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  ...
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]],
  [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   ...
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000],
   [1.00000000e+000, 1.00000000e+000, 1.00000000e+000 ... 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 2048, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:84

analyse output arrays in iter:261

pre layer res:
5:empty_merge_operator
{'name': 'empty_merge_operator', 'output': array([[[[1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
                     inf,            inf,            inf,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
                     inf,            inf,            inf,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00,            inf,
                     inf,            inf, 6.39843474e+17,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 2.71828175e+00,            inf,
                     inf,            inf, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf,            inf, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
                     inf,            inf,            inf,
                     inf, 3.26901750e+06, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 2.58131289e+20,
                     inf,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00,            inf,
                     inf,            inf,            inf,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 5.05239356e+31,            inf,
                     inf,            inf, 4.85165216e+08,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 2.71828175e+00,
          5.05239356e+31,            inf,            inf,
                     inf,            inf, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.31881574e+09,
                     inf,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          5.05239356e+31,            inf,            inf,
          1.31881574e+09, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00,            inf,
                     inf,            inf,            inf,
          9.49611953e+19, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 2.29378327e+27,
                     inf,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.17191425e+16,            inf,
                     inf,            inf,            inf,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 2.29378327e+27,            inf,
                     inf,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf,            inf, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.31881574e+09,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
                     inf, 4.85165216e+08, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf, 1.01480034e+33, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00,            inf,
                     inf,            inf,            inf,
          3.10429778e+26,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf, 1.58601345e+15, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00,            inf,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf,            inf,            inf,
          1.00000000e+00,            inf,            inf,
                     inf,            inf,            inf,
                     inf,            inf, 8.43835682e+26,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 9.49611953e+19,            inf,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
                     inf, 6.83767114e+30, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 6.39843474e+17,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
                     inf,            inf,            inf,
                     inf, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00,            inf,            inf,
                     inf,            inf,            inf,
                     inf, 9.49611953e+19, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00],
         [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00, 1.00000000e+00, 1.00000000e+00,
          1.00000000e+00]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [4, 4], 'to': [3]}
tf node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': TensorShape([1, 1, 28, 28]), 'from': [5], 'to': [2]}
ms node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan, 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan, 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        ,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan, 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan, 0.        ,        nan,
                 nan,        nan, 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ,        nan,        nan,        nan,
                 nan,        nan, 0.        ,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan, 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
                 nan,        nan,        nan,        nan,        nan,
                 nan, 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        ],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': (1, 1, 28, 28), 'from': [5], 'to': [2]}
torch node:
{'name': 'softmax', 'output': array([[[[0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [       nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan,        nan,        nan,
                 nan,        nan,        nan],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429],
         [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,
          0.03571429, 0.03571429, 0.03571429]]]], dtype=float32), 'output_shape': torch.Size([1, 1, 28, 28]), 'from': [5], 'to': [2]}

generate models:96

analyse output arrays in iter:263

pre layer res:
10:reshape
{'name': 'reshape', 'output': array([[-4.810314,  0.      ,  0.      , ...,  0.      ,  0.      ,
         0.      ]], dtype=float32), 'output_shape': TensorShape([1, 200704]), 'from': [9], 'to': [11]}
tf node:
{'name': 'log', 'output': array([[ nan, -inf, -inf, ..., -inf, -inf, -inf]], dtype=float32), 'output_shape': TensorShape([1, 200704]), 'from': [10], 'to': [6]}
ms node:
{'name': 'log', 'output': array([[ nan, -inf, -inf, ..., -inf, -inf, -inf]], dtype=float32), 'output_shape': (1, 200704), 'from': [10], 'to': [6]}
torch node:
{'name': 'log', 'output': array([[ nan, -inf, -inf, ..., -inf, -inf, -inf]], dtype=float32), 'output_shape': torch.Size([1, 200704]), 'from': [10], 'to': [6]}

generate models:97

analyse output arrays in iter:279

pre layer res:
20:reshape
{'name': 'reshape', 'output': array([[[[-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         ...,
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818]],

        [[-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         ...,
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818]],

        [[-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         [-16.661022, -16.661022, -16.661022, ...,       -inf,
          -16.661022, -16.661022],
         ...,
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818],
         [-13.328818, -13.328818, -13.328818, ...,       -inf,
          -13.328818, -13.328818]],

        ...,

        [[  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         ...,
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ]],

        [[  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         ...,
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ]],

        [[  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         ...,
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ],
         [  0.      ,   0.      ,   0.      , ...,   0.      ,
            0.      ,   0.      ]]]], dtype=float32), 'output_shape': TensorShape([1, 2048, 28, 28]), 'from': [19], 'to': [9]}
tf node:
{'name': 'conv2d', 'output': array([[[[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]],

        [[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]],

        [[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]],

        ...,

        [[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]],

        [[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]],

        [[-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         [-19188.,    -inf,    -inf, ...,    -inf,    -inf, -12792.],
         ...,
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-15354.,    -inf,    -inf, ...,    -inf,    -inf, -10236.],
         [-10236.,    -inf,    -inf, ...,    -inf,    -inf,  -6824.]]]],
      dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [20], 'to': []}
ms node:
{'name': 'conv2d', 'output': array([[[[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]],

        ...,

        [[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.443, -12795.678,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.678, -19193.55 ,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.557, -15354.836,       -inf, ...,       -inf,
                -inf,       -inf]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [20], 'to': []}
torch node:
{'name': 'conv2d', 'output': array([[[[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]],

        ...,

        [[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]],

        [[ -8530.485, -12795.735,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         [-12795.735, -19193.445,       -inf, ...,       -inf,
                -inf,       -inf],
         ...,
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf],
         [-10236.521, -15354.896,       -inf, ...,       -inf,
                -inf,       -inf]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [20], 'to': []}

pre layer res:
15:flatten
{'name': 'flatten', 'output': array([[0.03571429, 0.03571429, 0.03571429, ..., 0.03571429, 0.03571429,
        0.03571429]], dtype=float32), 'output_shape': TensorShape([1, 1605632]), 'from': [10], 'to': [21]}
tf node:
{'name': 'log', 'output': array([[-3.3322046, -3.3322046, -3.3322046, ..., -3.3322046, -3.3322046,
        -3.3322046]], dtype=float32), 'output_shape': TensorShape([1, 1605632]), 'from': [15], 'to': [16]}
ms node:
{'name': 'log', 'output': array([[-3.3322077, -3.3322077, -3.3322077, ..., -3.3322077, -3.3322077,
        -3.3322077]], dtype=float32), 'output_shape': (1, 1605632), 'from': [15], 'to': [16]}
torch node:
{'name': 'log', 'output': array([[-3.3322043, -3.3322043, -3.3322043, ..., -3.3322043, -3.3322043,
        -3.3322043]], dtype=float32), 'output_shape': torch.Size([1, 1605632]), 'from': [15], 'to': [16]}

generate models:104

analyse the exceptions in iter:307
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  45.,
            235., 147., 228., 254., 254., 160.,  38.,  84., 125.,  48.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 219.,
            214., 172., 253., 253., 253., 253., 250., 251., 253., 224.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 150., 252.,
            205.,  24., 117.,  76., 142., 142., 162., 253., 253., 170.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 228., 253.,
            218.,  27.,   0.,   0.,   0.,   0., 100., 253., 253.,  97.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 111., 253.,
            119.,   0.,   0.,   0.,   0.,  48., 246., 253., 173.,  10.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  63.,  48.,
              5.,   0.,   0.,   0.,   0., 166., 253., 253.,  91.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0., 183., 253., 240.,  60.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,  49., 241., 253., 174.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  19., 181., 253., 217.,  17.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0., 100., 253., 253., 174.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  48., 209., 253., 184.,  20.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  17., 213., 253., 241.,  57.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 103., 253., 241.,  93.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             57., 239., 253., 188.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             85., 253., 253., 171.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,
            169., 253., 246.,  52.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 169.,
            253., 253., 105.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  56., 240.,
            253., 209.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 165., 253.,
            251., 104.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 237.,
            168.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 1, 28, 28] to have 128 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 128, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:114

analyse output arrays in iter:317

pre layer res:
2:conv2d
{'name': 'conv2d', 'output': array([[[[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]],

        [[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]],

        [[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]],

        ...,

        [[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]],

        [[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]],

        [[11.741072, 63.375   , 28.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         ...,
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [15], 'to': [3]}
tf node:
{'name': 'conv2d', 'output': array([[[[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]],

        [[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]],

        [[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]],

        ...,

        [[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]],

        [[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]],

        [[ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,  19456.,      0.,      0.],
         [ 26398.,  46192.,  37288., ..., 133632.,      0.,      0.],
         ...,
         [ 26398.,  46192.,  99240., ...,      0.,      0.,      0.],
         [ 26398.,  46192.,  37288., ...,      0.,      0.,      0.],
         [     0.,      0.,      0., ...,      0.,      0.,      0.]]]],
      dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [2], 'to': [9]}
ms node:
{'name': 'conv2d', 'output': array([[[[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        ...,

        [[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.719,  39325.703,  53216.02 , ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [2], 'to': [9]}
torch node:
{'name': 'conv2d', 'output': array([[[[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        ...,

        [[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]],

        [[ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,  22528.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ..., 218368.   ,
               0.   ,      0.   ],
         ...,
         [     0.   ,      0.   , 115200.   , ...,      0.   ,
               0.   ,      0.   ],
         [ 19229.709,  39325.688,  53216.242, ...,      0.   ,
               0.   ,      0.   ],
         [     0.   ,      0.   ,      0.   , ...,      0.   ,
               0.   ,      0.   ]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [2], 'to': [9]}

generate models:117

analyse the exceptions in iter:323
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0., 382., 510., 128.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0., 382., 510., 510.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0., 128., 510., 510., 382.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 128., 510., 510., 382., 128.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0., 510., 510., 510., 128.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
           510., 510., 510., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 128.,
           510., 510., 382.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 128., 510.,
           510., 382., 128.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 382., 510.,
           510., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 382., 510., 510.,
           382.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 510., 510., 510.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 256., 510., 510., 382.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 510., 510., 510., 128.,
             0.,   0.,   0.,   0.,   0.,   0.,   0., 256., 256., 510., 256.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 256., 510., 510., 256.,   0.,
             0.,   0.,   0.,   0.,   0.,   0., 382., 510., 510., 510., 510.,
           128.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 256., 510., 510., 256.,   0.,
             0.,   0.,   0.,   0., 128., 510., 510., 510., 510., 510., 510.,
           382.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0., 128., 510., 510., 256.,   0.,
             0.,   0.,   0.,   0., 382., 510., 510., 382., 382., 510., 510.,
           382.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 382., 510., 382.,   0.,
             0.,   0.,   0., 256., 510., 510., 510., 382., 510., 510., 510.,
           128.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 256., 510., 510., 510.,
           256., 256., 256., 510., 510., 510., 510., 510., 510., 510., 256.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0., 128., 510., 510., 510.,
           510., 510., 510., 510., 510., 510., 510., 510., 510., 256.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 256., 510.,
           510., 510., 510., 510., 510., 510., 256., 128.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             0.,   0.,   0.,   0.,   0.,   0.]]]])]}
Given groups=1, weight of size [256, 1024, 3, 3], expected input[1, 1, 28, 28] to have 1024 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 1024, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:121

analyse output arrays in iter:326

pre layer res:
2:conv2d
{'name': 'conv2d', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [1], 'to': [8]}
tf node:
{'name': 'sin', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [2], 'to': [3]}
ms node:
{'name': 'sin', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': (1, 256, 28, 28), 'from': [2], 'to': [3]}
torch node:
{'name': 'sin', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 28]), 'from': [2], 'to': [3]}

generate models:123

analyse output arrays in iter:355

pre layer res:
19:transpose
{'name': 'transpose', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [11], 'to': [6]}
tf node:
{'name': 'sin', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [19], 'to': [4]}
ms node:
{'name': 'sin', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': (1, 256, 28, 28), 'from': [19], 'to': [4]}
torch node:
{'name': 'sin', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 28]), 'from': [19], 'to': [4]}

generate models:134

analyse output arrays in iter:358

pre layer res:
7:relu
{'name': 'relu', 'output': array([[[[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]],

        [[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]],

        [[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]],

        ...,

        [[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]],

        [[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]],

        [[67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         ...,
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95],
         [67918.95, 67918.95, 67918.95, ..., 67918.95, 67918.95,
          67918.95]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [6], 'to': [18]}
tf node:
{'name': 'sin', 'output': array([[[[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]],

        [[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]],

        [[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]],

        ...,

        [[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]],

        [[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]],

        [[-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         ...,
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105],
         [-0.75885105, -0.75885105, -0.75885105, ..., -0.75885105,
          -0.75885105, -0.75885105]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [7], 'to': [21]}
ms node:
{'name': 'sin', 'output': array([[[[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]],

        [[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]],

        [[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]],

        ...,

        [[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]],

        [[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]],

        [[-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         ...,
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851],
         [-0.758851, -0.758851, -0.758851, ..., -0.758851, -0.758851,
          -0.758851]]]], dtype=float32), 'output_shape': (1, 256, 28, 28), 'from': [7], 'to': [21]}
torch node:
{'name': 'sin', 'output': array([[[[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]],

        [[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]],

        [[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]],

        ...,

        [[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]],

        [[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]],

        [[-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         ...,
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395],
         [-0.76893395, -0.76893395, -0.76893395, ..., -0.76893395,
          -0.76893395, -0.76893395]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 28]), 'from': [7], 'to': [21]}

generate models:135

analyse output arrays in iter:372

pre layer res:
3:square
{'name': 'square', 'output': array([[[[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]],

        [[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]],

        [[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]],

        ...,

        [[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]],

        [[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]],

        [[9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         ...,
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.],
         [9., 9., 9., ..., 9., 9., 9.]]]], dtype=float32), 'output_shape': TensorShape([1, 128, 28, 28]), 'from': [11], 'to': [1]}
tf node:
{'name': 'conv2d', 'output': array([[[[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        ...,

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]]]],
      dtype=float32), 'output_shape': TensorShape([1, 64, 26, 26]), 'from': [3], 'to': [4, 4]}
ms node:
{'name': 'conv2d', 'output': array([[[[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        ...,

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]]]],
      dtype=float32), 'output_shape': (1, 64, 26, 26), 'from': [3], 'to': [4, 4]}
torch node:
{'name': 'conv2d', 'output': array([[[[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        ...,

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]],

        [[10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         ...,
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.],
         [10368., 10368., 10368., ..., 10368., 10368., 10368.]]]],
      dtype=float32), 'output_shape': torch.Size([1, 64, 26, 26]), 'from': [3], 'to': [4, 4]}

generate models:139

analyse output arrays in iter:393

pre layer res:
tf node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 447.,   0.,   0.],
         ...,
         [  0.,   0., 128., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [], 'to': [1, 8]}
ms node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [], 'to': [1, 8]}
torch node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ..., 383.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [], 'to': [1, 8]}

generate models:143

analyse output arrays in iter:403

pre layer res:
5:arctan
{'name': 'arctan', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [2], 'to': [6]}
tf node:
{'name': 'cos', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': TensorShape([1, 64, 28, 28]), 'from': [5], 'to': [3]}
ms node:
{'name': 'cos', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': (1, 64, 28, 28), 'from': [5], 'to': [3]}
torch node:
{'name': 'cos', 'output': array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]],

        [[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         ...,
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32), 'output_shape': torch.Size([1, 64, 28, 28]), 'from': [5], 'to': [3]}

generate models:146

analyse output arrays in iter:414

pre layer res:
5:log
{'name': 'log', 'output': array([[[[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        ...,

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]],

        [[-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         ...,
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf],
         [-inf, -inf, -inf, ..., -inf, -inf, -inf]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [9], 'to': [10]}
tf node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 28, 28]), 'from': [5], 'to': [8]}
ms node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': (1, 256, 28, 28), 'from': [5], 'to': [8]}
torch node:
{'name': 'log', 'output': array([[[[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        ...,

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]],

        [[nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         ...,
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan],
         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 28, 28]), 'from': [5], 'to': [8]}

generate models:152

analyse the exceptions in iter:423
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  41., 137., 237., 203., 131.,  16.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  21., 111., 225., 254., 254., 254., 254., 243.,  54.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
             30., 218., 254., 217., 165.,  83., 190., 253., 254.,  89.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31.,
            217., 254., 190.,  20.,   0.,   0.,   0., 128., 254.,  89.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 149.,
            254., 246.,  62.,   0.,   0.,   0.,   0.,  54., 254.,  89.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 129., 251.,
            252.,  79.,   0.,   0.,   0.,   0., 101., 241., 228.,  10.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 208., 254.,
            249.,   0.,   0.,   0.,   0.,  50., 238., 254., 164.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 141., 254.,
            252., 142.,  68., 108., 142., 237., 254., 243.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  33., 217.,
            254., 254., 254., 254., 254., 254., 254., 148.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  33.,
            134., 195., 121., 119., 254., 255., 175.,   2.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,  96., 254., 254., 106.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,  80., 242., 254., 130.,   4.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,  11., 202., 254., 254.,  41.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              1.,  91., 254., 252., 116.,   1.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              6., 254., 254., 195.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            107., 254., 247., 104.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  78.,
            252., 254., 127.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 146.,
            254., 197.,  20.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 226.,
            252.,  68.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 170.,
            249.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [2048, 2048, 1, 1], expected input[1, 1, 28, 28] to have 2048 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 2048, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:157

analyse output arrays in iter:424

pre layer res:
tf node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 257.,   0.,   0.],
         [  0.,   0.,   0., ..., 164.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]], dtype=float32), 'output_shape': TensorShape([1, 256, 14, 14]), 'from': [], 'to': [1]}
ms node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]]]], dtype=float32), 'output_shape': (1, 256, 14, 14), 'from': [], 'to': [1]}
torch node:
{'name': 'conv2d', 'output': array([[[[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        ...,

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]],

        [[  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         [  0.,   0.,   0., ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0., ..., 667.,   0.,   0.],
         [  0.,   0.,   0., ..., 865.,   0.,   0.],
         [  0.,   0.,   0., ..., 104.,   0.,   0.]]]], dtype=float32), 'output_shape': torch.Size([1, 256, 14, 14]), 'from': [], 'to': [1]}

generate models:158

analyse the exceptions in iter:425
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<ConvolutionBackward0>)]}
Given groups=1, weight of size [256, 1024, 3, 3], expected input[1, 256, 28, 28] to have 1024 channels, but got 256 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 256, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  ...
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]],
  [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 1024, but got 'C_in' of input 'x' shape: 256, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:159

analyse the exceptions in iter:429
torch exception:
{'id': 0, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': tensor([[[[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0., 218., 253., 253., 255., 149.,  62.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  42.,
            144., 236., 251., 251., 253., 251., 236., 144., 144.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  99.,
            251., 251., 251., 225., 253., 251., 251., 251., 251., 166.,  16.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79., 253.,
            251., 251., 204.,  41., 143., 205., 251., 251., 251., 253., 169.,
             15.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79., 231., 253.,
            251., 225.,  41.,   0.,   0.,  41., 226., 251., 251., 253., 251.,
            164.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  37., 253., 253., 255.,
            253.,  35.,   0.,   0.,   0.,   0.,   0.,  79., 232., 255., 253.,
            227.,  42.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0., 140., 251., 251., 253.,
            168.,  15.,   0.,   0.,   0.,   0.,   0.,   0.,  77., 253., 251.,
            251., 142.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  21., 221., 251., 251., 164.,
             15.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 227., 251.,
            251., 236.,  61.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  32., 190., 251., 251., 251.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73., 251.,
            251., 251.,  71.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  73., 251., 251., 251., 251.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73., 251.,
            251., 251.,  71.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  73., 253., 253., 253., 201.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73., 253.,
            253., 253.,  72.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 176., 251., 251., 251.,  71.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73., 251.,
            251., 251.,  71.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 253., 251., 251., 157.,  10.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  73., 251.,
            251., 251.,  71.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 253., 251., 251., 142.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 150., 251.,
            251., 204.,  41.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0., 124., 251., 251., 220., 180.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 130., 253., 251.,
            225.,  41.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  73., 253., 253., 253., 253.,  73.,
             73.,  10.,   0.,   0.,   0.,  42.,  73., 150., 253., 255., 253.,
            216.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,  31., 189., 251., 251., 251., 253.,
            251., 159., 144., 144., 145., 206., 251., 251., 251., 253., 168.,
             92.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,  20., 195., 251., 251., 253.,
            251., 251., 251., 251., 253., 251., 251., 251., 225., 164.,  15.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 142., 220., 253.,
            251., 251., 251., 251., 253., 251., 251., 204.,  41.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,  72.,
            174., 251., 251., 251., 253., 147.,  71.,  41.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
              0.,   0.,   0.,   0.,   0.,   0.]]]]])}
Given groups=1, weight of size [256, 256, 1, 1], expected input[1, 1, 28, 28] to have 256 channels, but got 1 channels instead
mindspore exception:
{'id': 0, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': Tensor(shape=[1, 1, 1, 28, 28], dtype=Float32, value=
[[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    ...
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
    [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]]])}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 256, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:160

analyse the exceptions in iter:453
torch exception:
{'id': 1, 'name': 'conv2d', 'frame_work': 'torch', 'input_datas': [tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0.]]]])]}
Given groups=1, weight of size [512, 1024, 1, 1], expected input[1, 1, 28, 28] to have 1024 channels, but got 1 channels instead
mindspore exception:
{'id': 1, 'name': 'conv2d', 'framework': 'mindspore', 'input_datas': [Tensor(shape=[1, 1, 28, 28], dtype=Float32, value=
[[[[0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   ...
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],
   [0.00000000e+000, 0.00000000e+000, 0.00000000e+000 ... 0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]]])]}
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 1024, but got 'C_in' of input 'x' shape: 1, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\conv2d.cc:214 Conv2dInferShape


generate models:170

analyse output arrays in iter:463

pre layer res:
0:conv2d
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 1024, 28, 28]), 'from': [], 'to': [1]}
tf node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': TensorShape([1, 512, 28, 28]), 'from': [0], 'to': [2, 5]}
ms node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': (1, 512, 28, 28), 'from': [0], 'to': [2, 5]}
torch node:
{'name': 'conv2d', 'output': array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]],

        [[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         ...,
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), 'output_shape': torch.Size([1, 512, 28, 28]), 'from': [0], 'to': [2, 5]}

generate models:177

final statics:
total operators:28
tensorflow --> nums:19,distinct_bugs:5
mindspore --> nums:23,distinct_bugs:4
torch --> nums:23,distinct_bugs:5
tensorflow --> 
conv2d:10
softmax:5
log:2
sin:1
cos:1
mindspore --> 
softmax:5
conv2d:13
log:3
sin:2
torch --> 
softmax:5
conv2d:13
flatten:1
log:2
sin:2

generate models:186
